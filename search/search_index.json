{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Shaken Fist, a minimal cloud aimed at small and edge deployments Shaken Fist is a deliberately minimal cloud intended for small deployments. We spend a lot of time trying to do the simplest possible thing well, and keep our resource usage on idle deployments as low as possible. Shaken Fist has progressed from being a proof of concept to being a functional cloud, although the release numbers being below zero indicates that we are still stabilizing the REST API and that breaking changes might occur. 1.0 will be released when we are relatively sure of stability going forwards. Shaken Fist started a personal research project, but has grown into something which is starting to see wider contributions and some small commercial deployments. The underlying idea Originally Shaken Fist came about as a reaction to the increasing complexity of OpenStack, as well as a desire to experiment with alternative approaches to solving the problems that OpenStack Compute addresses. What I really wanted was a simple API to orchestrate virtual machines, but it needed to run with minimal resource overhead and be simple to deploy. I also wanted it to always work in a predictable way. One of the reasons OpenStack is so complicated and its behaviour varies is because it has many options to configure. The solution seemed obvious to me -- a cloud that is super opinionated. For each different functional requirement there is one option, and the simplest option is chosen where possible. Read on for some examples. Development choices If there is an existing library which does a thing, we use it. OpenStack suffered from being old (and having issues with re-writes being hard), as well as licensing constraints. We just use the code that others have provided to the community. Always. Deployment choices libvirt is the only supported hypervisor. Instances are specified to libvirt with simple templated XML. If your local requirements are different to what's in the template, you're welcome to change the template to meet your needs. If your template changes break things, you're also welcome to debug what went wrong for yourself. \"I'd like to subscribe to your newsletter\" First off, we don't have a newsletter. That said, updates will be announced here as they happen. What we do have is useful links: The Shaken Fist repository is where the code for the server is, including the deployer. Its a good place to start. There is also the python client repository contains the python API client, as well as the command line client that users and shell scripts use to interact with Shaken Fist. What is Shaken Fist? Can I help? The Shaken Fist Manifesto is our general conceptual starting point, ubt apart from that just reach out and have a chat.","title":"Introduction"},{"location":"#introduction","text":"","title":"Introduction"},{"location":"#shaken-fist-a-minimal-cloud-aimed-at-small-and-edge-deployments","text":"Shaken Fist is a deliberately minimal cloud intended for small deployments. We spend a lot of time trying to do the simplest possible thing well, and keep our resource usage on idle deployments as low as possible. Shaken Fist has progressed from being a proof of concept to being a functional cloud, although the release numbers being below zero indicates that we are still stabilizing the REST API and that breaking changes might occur. 1.0 will be released when we are relatively sure of stability going forwards. Shaken Fist started a personal research project, but has grown into something which is starting to see wider contributions and some small commercial deployments.","title":"Shaken Fist, a minimal cloud aimed at small and edge deployments"},{"location":"#the-underlying-idea","text":"Originally Shaken Fist came about as a reaction to the increasing complexity of OpenStack, as well as a desire to experiment with alternative approaches to solving the problems that OpenStack Compute addresses. What I really wanted was a simple API to orchestrate virtual machines, but it needed to run with minimal resource overhead and be simple to deploy. I also wanted it to always work in a predictable way. One of the reasons OpenStack is so complicated and its behaviour varies is because it has many options to configure. The solution seemed obvious to me -- a cloud that is super opinionated. For each different functional requirement there is one option, and the simplest option is chosen where possible. Read on for some examples.","title":"The underlying idea"},{"location":"#development-choices","text":"If there is an existing library which does a thing, we use it. OpenStack suffered from being old (and having issues with re-writes being hard), as well as licensing constraints. We just use the code that others have provided to the community. Always.","title":"Development choices"},{"location":"#deployment-choices","text":"libvirt is the only supported hypervisor. Instances are specified to libvirt with simple templated XML. If your local requirements are different to what's in the template, you're welcome to change the template to meet your needs. If your template changes break things, you're also welcome to debug what went wrong for yourself.","title":"Deployment choices"},{"location":"#id-like-to-subscribe-to-your-newsletter","text":"First off, we don't have a newsletter. That said, updates will be announced here as they happen. What we do have is useful links: The Shaken Fist repository is where the code for the server is, including the deployer. Its a good place to start. There is also the python client repository contains the python API client, as well as the command line client that users and shell scripts use to interact with Shaken Fist.","title":"\"I'd like to subscribe to your newsletter\""},{"location":"#what-is-shaken-fist-can-i-help","text":"The Shaken Fist Manifesto is our general conceptual starting point, ubt apart from that just reach out and have a chat.","title":"What is Shaken Fist? Can I help?"},{"location":"community/","text":"Welcome to the Shaken Fist community First off, Shaken Fist is an open source project covered by the Apache2 license. The community is relatively small at this point and we're still working out what works for us. For now, we communicate through the following mechanisms: GitHub issues and pull requests in the various repositories. A slack workspace at https://shakenfist.slack.com -- unfortunately you need to be invited to that workspace, but we're happy to do that. If you're interested, please email mikal@stillhq.com . Supporters We try to track supporters here. Our apologies if we've missed someone, let us know and we'll fix it. Michael Still and Andrew McCallum have contributed significant personal time to the project. Aptira has donated developer time, as well as provided invaluable feedback from the largest real world deployments that we've had so far. FifthDomain has also donated developer time. Shaken Fist was a recipient of a 2020 Icculus micro grant .","title":"Community"},{"location":"community/#welcome-to-the-shaken-fist-community","text":"First off, Shaken Fist is an open source project covered by the Apache2 license. The community is relatively small at this point and we're still working out what works for us. For now, we communicate through the following mechanisms: GitHub issues and pull requests in the various repositories. A slack workspace at https://shakenfist.slack.com -- unfortunately you need to be invited to that workspace, but we're happy to do that. If you're interested, please email mikal@stillhq.com .","title":"Welcome to the Shaken Fist community"},{"location":"community/#supporters","text":"We try to track supporters here. Our apologies if we've missed someone, let us know and we'll fix it. Michael Still and Andrew McCallum have contributed significant personal time to the project. Aptira has donated developer time, as well as provided invaluable feedback from the largest real world deployments that we've had so far. FifthDomain has also donated developer time. Shaken Fist was a recipient of a 2020 Icculus micro grant .","title":"Supporters"},{"location":"features/","text":"Feature matrix What features does Shaken Fist have now? What about in the future? This page attempts to document the currently implemented features, but it is a bit a moving target. If you're left wondering if something works, please reach out to us and ask. High level functionality Our high level functionality is why you'd consider using Shaken Fist. Specifically, we support: instances: which are virtual machines deployed and managed by Shaken Fist. virtual networks: which are VXLAN meshes between hypervisors managed by Shaken Fist. These virtual networks do automatic IP address management, optionally provide DHCP and NAT, and support floating IPs for external accessability. resource efficiency: we try hard to not use much in terms of resources in an orchestration idle state (that is, your workload isn't changing), but we also deploy and configure Kernel Shared Memory (KSM) and make heavy use of qcow2 Copy On Write (COW) layers to reduce the resources used by a single instance. This means you can pack more instances onto a Shaken Fist cluster than you can alternative deployments of the same size from other projects. Object types We also have a lot of implementation functionality that is quite useful, but not the sort of thing you'd put on a billboard. Let's work through that by object type. Artifacts Artifacts are Shaken Fist's object type for disk images -- the sort of thing that you would store in Glance in OpenStack. Artifacts can store downloaded disk images from the internet (the \"image\" type), snapshots of previous instances (the \"snapshot\" type), and arbitrary uploads (also stored with the \"image\" type). There is also a special \"label\" artifact type, which is an overlay on top of the other types. Its easiest to explain its behavior by explaining the lifecycle of an artifact. The normal way to get your first artifact is to download something from the internet. So for example, you might start an instance with a standard Ubuntu cloud image from https://cloud-images.ubuntu.com/ . This would be done by specifying the URL of the image in the disk specification of the image: Shaken Fist will then download the image and store it as an artifact, and then start your instance. A second instance using the same image will then check the image at the URL hasn't changed, and if it hasn't use the same artifact as the first instance, skipping the repeated download. However, if the image has changed, a second version would be downloaded. Depending on the settings for the artifact, both versions are retained. By default Shaken Fist keeps the last three versions of each artifact, although this is configurable. Now let's assume that you have a nightly CI job which starts an instance from the latest Ubuntu cloud image, and performs some tests to ensure that it works for your software stack. You want to somehow mark for your other workloads what versions of the Ubuntu image are trusted, and you do this with a label. So, your CI job would specify the upstream URL for the cloud image, perform its tests, and then label the image if it passed those tests. Other Ubuntu users in your cloud could then specify that they wanted the most recent version which passed testing by specifying the label for their disk specification, instead of the upstream URL. Shaken Fist's CI does exactly this. Each night we download a set of cloud images, customize them to make the CI runs a bit faster (pre-installing packages and so forth), and then test that they work. At the end of that run we take a snapshot of the instance we customized, and label it with a label along the lines of \"sfci-ubuntu-2004\". CI jobs then use that label for their base disk. You can see the ansible we use to do this at https://github.com/shakenfist/shakenfist/blob/develop/deploy/ansible/ci-image.yml if you're interested. The following operations are exposed on artifacts by the REST API: Operation Command line client API client list artifacts artifact list get_artifacts() show an artifact artifact show get_artifact() fetch an artifact from a URL without starting an instance artifact cache cache_artifact() upload artifact upload create_upload() followed by calls to send_upload() and then upload_artifact() download artifact download lookup the desired version's blob with get_artifact() , then download with get_blob_data() show detailed information about versions artifact versions get_artifact_versions() delete artifact delete delete_artifact() delete a version artifact delete-version delete_artifact_version() set the maximum number of versions artifact max-versions set_artifact_max_versions() Note that artifacts exist in namespaces (since v0.6). This means that your artifacts are private to your namespace, and can't be seen or used by other namespaces. There are two exceptions -- the \"system\" administrative namespace can see all artifacts, and the \"system\" namespace can create artifacts visible to all other namespaces -- this is done with the shared flag on the relevant command line or API calls, and uses a \"sharedwithall\" namespace in the database. Blobs Each version of an artifact is an object called a blob. Blobs are stored on Shaken Fist nodes, and are automatically replicated around the cluster as required. By default we store at least two copies of each blob, although this is configurable. Its possible we'll store a lot more copies than that, because we only reap excess copies when we start to run low on disk. This is because these blobs are often used during the startup of instances, so having a local cache of popular blobs can significantly improve instance start up times. All hypervisor nodes store blobs, but it is also possible to have \"storage only\" nodes which don't run VMs and just store blobs. In previous deployments we have used these storage nodes to handle having more blobs than we need for currently running instances -- for example historical snapshots we are fond of, but are unlikely to require frequent access to. The storage nodes were therefore a cheaper machine type with slower CPU and disk, but a lot more disk than our hypervisor nodes. So for example if you had an edge deployment where you are resource constrained, but also want to take nightly instance snapshots as a backup, you might have a more centrally located storage node and Shaken Fist would migrate unused blobs there to free up space on the edge nodes as required. If a blob only present on a storage only node is required for an instance start, a hypervisor node will fetch it at that time. Finally, blobs are reference counted. They can be used by more than one artifact (for example an image which is then labelled), and we also count how many instances are using a specific blob. We only delete a blob from disk when there are no remaining references to it. The following operations are exposed on blobs by the REST API: Operation Command line client API client list blobs blob list get_blobs() Events Shaken Fist has an event logging system for the main object types. So for example, instead of reading through log files to find all the state changes that an instance went through, you can simply ask for a list of the events for that instance. This also means that the instance owner can see those logs without having to be given access to your log files. The following object types currently record events: artifacts; blobs; instances; networks; networkinterfaces; nodes; and uploads. In general, events are exposed in the API as operations on the object they relate to. So for example there is a instance events command, which calls the get_instance_events() API client call. Those various calls are documented by their object type. Networks Note that networks exist in namespaces. This means that your networks are private to your namespace, and can't be seen or used by other namespaces. There is one exception -- the \"system\" administrative namespace can see all networks. Instances Instances are the primary reason that you'd run Shaken Fist, so there's a lot to cover in their implementation. Obviously instances can be created, deleted, listed, and shown. Additionally, you can list the network interfaces on an instance, track and change metadata on a given instance (a simple key value store similar to OpenStack tags), request the current serial console output; and see events related to the instance. Instances can also have their power state managed: soft (ACPI) reboots; hard (power cycle) reboots; powered off; powered on; and paused. When creating an instance you can configure: * the name of the instance * how many vCPUs the instance has * how much memory the instance has * what network connections the instance has, including floating IP attachments and the network interface model to use * what disks the instance has, their size, type, and bus * what ssh key cloud-init should set up, if your instance includes cloud-init * other arbitrary user data which will be passed to cloud-init, if installed * the namespace of the instance * what video card the instance has, including the model and amount of video memory * whether BIOS boot or UEFI boot is used * whether secure boot is enabled, including a NVRAM template if required * what configuration drive type is used, with a default of OpenStack style * key and value metadata Note that instances exist in namespaces. This means that your instances are private to your namespace, and can't be seen or used by other namespaces. There is one exception -- the \"system\" administrative namespace can see all instances. Other features Shaken Fist supports the follow other features that are not directly related to an object type: JWT based API authentication graceful shutdown of hypervisors where current work is finished before the processes are stopped online upgrade of object versions as required Comparison to OpenStack The development team's background is OpenStack, so we find it useful to provide a comparison between what OpenStack supports and what Shaken Fist supports. However, Shaken Fist does not intend to be a direct replacement for OpenStack, and implements many features not present in OpenStack (for example in guest agents). Here's a simple feature matrix listing when a feature was introduced: Feature Implemented Planned Not Planned Servers / instances v0.1 Networks v0.1 Multiple NIC's for a given server v0.1 Pre-cache a server image v0.1 Floating IPs v0.1 Pause v0.1 Reboot (hard and soft) v0.1 Security groups Yes Text console v0.1 VDI v0.1 User data v0.1 Keypairs v0.1 Virtual networks allow overlapping IP allocations v0.1 REST API authentication and object ownership v0.2 Snapshots (of all disks) v0.1 Central API service v0.1 Scheduling v0.1 Volumes No plans Quotas No plans API versioning No plans Keystone style service lookup and URLs No plans Create multiple servers in a single request No plans Resize a server No plans Server groups No plans Change admin password No plans Rebuild a server No plans Shelve / unshelve No plans Trigger crash dump No plans Live migration No plans Flavors No plans Guest agents No plans Host aggregates No plans Server tags v0.2, we call them \"metadata\" ~~Persistence in MySQL~~ v0.1 Distributed etcd for locking and persistence v0.2 Production grade REST API via gunicorn v0.2 Python REST API client v0.1 golang REST API client v0.2 Terraform provider v0.2","title":"Features"},{"location":"features/#feature-matrix","text":"What features does Shaken Fist have now? What about in the future? This page attempts to document the currently implemented features, but it is a bit a moving target. If you're left wondering if something works, please reach out to us and ask.","title":"Feature matrix"},{"location":"features/#high-level-functionality","text":"Our high level functionality is why you'd consider using Shaken Fist. Specifically, we support: instances: which are virtual machines deployed and managed by Shaken Fist. virtual networks: which are VXLAN meshes between hypervisors managed by Shaken Fist. These virtual networks do automatic IP address management, optionally provide DHCP and NAT, and support floating IPs for external accessability. resource efficiency: we try hard to not use much in terms of resources in an orchestration idle state (that is, your workload isn't changing), but we also deploy and configure Kernel Shared Memory (KSM) and make heavy use of qcow2 Copy On Write (COW) layers to reduce the resources used by a single instance. This means you can pack more instances onto a Shaken Fist cluster than you can alternative deployments of the same size from other projects.","title":"High level functionality"},{"location":"features/#object-types","text":"We also have a lot of implementation functionality that is quite useful, but not the sort of thing you'd put on a billboard. Let's work through that by object type.","title":"Object types"},{"location":"features/#artifacts","text":"Artifacts are Shaken Fist's object type for disk images -- the sort of thing that you would store in Glance in OpenStack. Artifacts can store downloaded disk images from the internet (the \"image\" type), snapshots of previous instances (the \"snapshot\" type), and arbitrary uploads (also stored with the \"image\" type). There is also a special \"label\" artifact type, which is an overlay on top of the other types. Its easiest to explain its behavior by explaining the lifecycle of an artifact. The normal way to get your first artifact is to download something from the internet. So for example, you might start an instance with a standard Ubuntu cloud image from https://cloud-images.ubuntu.com/ . This would be done by specifying the URL of the image in the disk specification of the image: Shaken Fist will then download the image and store it as an artifact, and then start your instance. A second instance using the same image will then check the image at the URL hasn't changed, and if it hasn't use the same artifact as the first instance, skipping the repeated download. However, if the image has changed, a second version would be downloaded. Depending on the settings for the artifact, both versions are retained. By default Shaken Fist keeps the last three versions of each artifact, although this is configurable. Now let's assume that you have a nightly CI job which starts an instance from the latest Ubuntu cloud image, and performs some tests to ensure that it works for your software stack. You want to somehow mark for your other workloads what versions of the Ubuntu image are trusted, and you do this with a label. So, your CI job would specify the upstream URL for the cloud image, perform its tests, and then label the image if it passed those tests. Other Ubuntu users in your cloud could then specify that they wanted the most recent version which passed testing by specifying the label for their disk specification, instead of the upstream URL. Shaken Fist's CI does exactly this. Each night we download a set of cloud images, customize them to make the CI runs a bit faster (pre-installing packages and so forth), and then test that they work. At the end of that run we take a snapshot of the instance we customized, and label it with a label along the lines of \"sfci-ubuntu-2004\". CI jobs then use that label for their base disk. You can see the ansible we use to do this at https://github.com/shakenfist/shakenfist/blob/develop/deploy/ansible/ci-image.yml if you're interested. The following operations are exposed on artifacts by the REST API: Operation Command line client API client list artifacts artifact list get_artifacts() show an artifact artifact show get_artifact() fetch an artifact from a URL without starting an instance artifact cache cache_artifact() upload artifact upload create_upload() followed by calls to send_upload() and then upload_artifact() download artifact download lookup the desired version's blob with get_artifact() , then download with get_blob_data() show detailed information about versions artifact versions get_artifact_versions() delete artifact delete delete_artifact() delete a version artifact delete-version delete_artifact_version() set the maximum number of versions artifact max-versions set_artifact_max_versions() Note that artifacts exist in namespaces (since v0.6). This means that your artifacts are private to your namespace, and can't be seen or used by other namespaces. There are two exceptions -- the \"system\" administrative namespace can see all artifacts, and the \"system\" namespace can create artifacts visible to all other namespaces -- this is done with the shared flag on the relevant command line or API calls, and uses a \"sharedwithall\" namespace in the database.","title":"Artifacts"},{"location":"features/#blobs","text":"Each version of an artifact is an object called a blob. Blobs are stored on Shaken Fist nodes, and are automatically replicated around the cluster as required. By default we store at least two copies of each blob, although this is configurable. Its possible we'll store a lot more copies than that, because we only reap excess copies when we start to run low on disk. This is because these blobs are often used during the startup of instances, so having a local cache of popular blobs can significantly improve instance start up times. All hypervisor nodes store blobs, but it is also possible to have \"storage only\" nodes which don't run VMs and just store blobs. In previous deployments we have used these storage nodes to handle having more blobs than we need for currently running instances -- for example historical snapshots we are fond of, but are unlikely to require frequent access to. The storage nodes were therefore a cheaper machine type with slower CPU and disk, but a lot more disk than our hypervisor nodes. So for example if you had an edge deployment where you are resource constrained, but also want to take nightly instance snapshots as a backup, you might have a more centrally located storage node and Shaken Fist would migrate unused blobs there to free up space on the edge nodes as required. If a blob only present on a storage only node is required for an instance start, a hypervisor node will fetch it at that time. Finally, blobs are reference counted. They can be used by more than one artifact (for example an image which is then labelled), and we also count how many instances are using a specific blob. We only delete a blob from disk when there are no remaining references to it. The following operations are exposed on blobs by the REST API: Operation Command line client API client list blobs blob list get_blobs()","title":"Blobs"},{"location":"features/#events","text":"Shaken Fist has an event logging system for the main object types. So for example, instead of reading through log files to find all the state changes that an instance went through, you can simply ask for a list of the events for that instance. This also means that the instance owner can see those logs without having to be given access to your log files. The following object types currently record events: artifacts; blobs; instances; networks; networkinterfaces; nodes; and uploads. In general, events are exposed in the API as operations on the object they relate to. So for example there is a instance events command, which calls the get_instance_events() API client call. Those various calls are documented by their object type.","title":"Events"},{"location":"features/#networks","text":"Note that networks exist in namespaces. This means that your networks are private to your namespace, and can't be seen or used by other namespaces. There is one exception -- the \"system\" administrative namespace can see all networks.","title":"Networks"},{"location":"features/#instances","text":"Instances are the primary reason that you'd run Shaken Fist, so there's a lot to cover in their implementation. Obviously instances can be created, deleted, listed, and shown. Additionally, you can list the network interfaces on an instance, track and change metadata on a given instance (a simple key value store similar to OpenStack tags), request the current serial console output; and see events related to the instance. Instances can also have their power state managed: soft (ACPI) reboots; hard (power cycle) reboots; powered off; powered on; and paused. When creating an instance you can configure: * the name of the instance * how many vCPUs the instance has * how much memory the instance has * what network connections the instance has, including floating IP attachments and the network interface model to use * what disks the instance has, their size, type, and bus * what ssh key cloud-init should set up, if your instance includes cloud-init * other arbitrary user data which will be passed to cloud-init, if installed * the namespace of the instance * what video card the instance has, including the model and amount of video memory * whether BIOS boot or UEFI boot is used * whether secure boot is enabled, including a NVRAM template if required * what configuration drive type is used, with a default of OpenStack style * key and value metadata Note that instances exist in namespaces. This means that your instances are private to your namespace, and can't be seen or used by other namespaces. There is one exception -- the \"system\" administrative namespace can see all instances.","title":"Instances"},{"location":"features/#other-features","text":"Shaken Fist supports the follow other features that are not directly related to an object type: JWT based API authentication graceful shutdown of hypervisors where current work is finished before the processes are stopped online upgrade of object versions as required","title":"Other features"},{"location":"features/#comparison-to-openstack","text":"The development team's background is OpenStack, so we find it useful to provide a comparison between what OpenStack supports and what Shaken Fist supports. However, Shaken Fist does not intend to be a direct replacement for OpenStack, and implements many features not present in OpenStack (for example in guest agents). Here's a simple feature matrix listing when a feature was introduced: Feature Implemented Planned Not Planned Servers / instances v0.1 Networks v0.1 Multiple NIC's for a given server v0.1 Pre-cache a server image v0.1 Floating IPs v0.1 Pause v0.1 Reboot (hard and soft) v0.1 Security groups Yes Text console v0.1 VDI v0.1 User data v0.1 Keypairs v0.1 Virtual networks allow overlapping IP allocations v0.1 REST API authentication and object ownership v0.2 Snapshots (of all disks) v0.1 Central API service v0.1 Scheduling v0.1 Volumes No plans Quotas No plans API versioning No plans Keystone style service lookup and URLs No plans Create multiple servers in a single request No plans Resize a server No plans Server groups No plans Change admin password No plans Rebuild a server No plans Shelve / unshelve No plans Trigger crash dump No plans Live migration No plans Flavors No plans Guest agents No plans Host aggregates No plans Server tags v0.2, we call them \"metadata\" ~~Persistence in MySQL~~ v0.1 Distributed etcd for locking and persistence v0.2 Production grade REST API via gunicorn v0.2 Python REST API client v0.1 golang REST API client v0.2 Terraform provider v0.2","title":"Comparison to OpenStack"},{"location":"manifesto/","text":"The Shaken Fist Manifesto This document attempts to list Shaken Fist's defining features, give guidance on what type of features should be added to the project, how they should be implemented and how we work together. Shaken Fist Defining Characteristics Shaken Fist is smaller, simpler cloud. It is designed for relatively small environments with minimum management overhead. Its features are highly opinionated. This means that the maintainers have chosen the best (in their opinion) features to support. Opinionated features do not handle every single possible use case. This reduces the code base size thus increasing long-term maintainability. The code base is understandable in its entirety by a single developer. A Shaken Fist cluster does not need a team of engineers to install or operate. A Shaken Fist cluster should be simple to set up. We define 'simple' as \"a person with no knowledge of the project can build a reasonable cluster in an evening\". Project Goals Allow simple management of virtual machine instances without complexity. Support networking between those machines and also facilitate access to external networks. Avoid re-inventing the wheel (utilise other open source projects when appropriate). Feature Guidelines Features should be deliberately limited in the options available. The goal of limiting options is to reduce code complexity. If the option does not add significant code complexity then it should added. The supported features and the options of those features should aim to cover the majority of use cases. When a feature limits the available options, it should do so in a way that does not overly restrict a project fork from adding that option. New code should conform to the conventions of the existing code base and written to be easily understood. New code should have new tests (please). Significant Opinionated Design Decisions The only supported hypervisor is KVM managed by libvirt. Virtual networking is only implemented via VXLAN meshes. Single machine clusters should always be possible. Only the current Ubuntu LTS version and Debian supported by the main project (pull requests to support other operating systems are encouraged). Project Interaction Guidelines Always polite. Always generous. Being opinionated is encouraged (but gently). Updating the documentation is just as important as the code change itself. Developers who write tests are the most highly prized of all the developers.","title":"Manifesto"},{"location":"manifesto/#the-shaken-fist-manifesto","text":"This document attempts to list Shaken Fist's defining features, give guidance on what type of features should be added to the project, how they should be implemented and how we work together.","title":"The Shaken Fist Manifesto"},{"location":"manifesto/#shaken-fist-defining-characteristics","text":"Shaken Fist is smaller, simpler cloud. It is designed for relatively small environments with minimum management overhead. Its features are highly opinionated. This means that the maintainers have chosen the best (in their opinion) features to support. Opinionated features do not handle every single possible use case. This reduces the code base size thus increasing long-term maintainability. The code base is understandable in its entirety by a single developer. A Shaken Fist cluster does not need a team of engineers to install or operate. A Shaken Fist cluster should be simple to set up. We define 'simple' as \"a person with no knowledge of the project can build a reasonable cluster in an evening\".","title":"Shaken Fist Defining Characteristics"},{"location":"manifesto/#project-goals","text":"Allow simple management of virtual machine instances without complexity. Support networking between those machines and also facilitate access to external networks. Avoid re-inventing the wheel (utilise other open source projects when appropriate).","title":"Project Goals"},{"location":"manifesto/#feature-guidelines","text":"Features should be deliberately limited in the options available. The goal of limiting options is to reduce code complexity. If the option does not add significant code complexity then it should added. The supported features and the options of those features should aim to cover the majority of use cases. When a feature limits the available options, it should do so in a way that does not overly restrict a project fork from adding that option. New code should conform to the conventions of the existing code base and written to be easily understood. New code should have new tests (please).","title":"Feature Guidelines"},{"location":"manifesto/#significant-opinionated-design-decisions","text":"The only supported hypervisor is KVM managed by libvirt. Virtual networking is only implemented via VXLAN meshes. Single machine clusters should always be possible. Only the current Ubuntu LTS version and Debian supported by the main project (pull requests to support other operating systems are encouraged).","title":"Significant Opinionated Design Decisions"},{"location":"manifesto/#project-interaction-guidelines","text":"Always polite. Always generous. Being opinionated is encouraged (but gently). Updating the documentation is just as important as the code change itself. Developers who write tests are the most highly prized of all the developers.","title":"Project Interaction Guidelines"},{"location":"developer_guide/artifact_uploads_downloads/","text":"Artifact uploads and downloads The general usage of artifact uploads and downloads is documented in the user guide . This page documents the actual API flow for uploading or downloading an artifact and is only useful to developers implementing new Shaken Fist clients. Uploads Artifact uploads normally require multiple HTTP requests in order to complete. This is because artifacts are often very large, and the REST API wants to allow you to continue an upload even if a single HTTP session fails or times out. This is implemented by creating an upload object, POSTing data to that object repeatedly, and then converting that upload object to an artifact. Upload objects which have not have data posted to them in a long time (currently 24 hours) are automatically removed. You create an upload by POST'ing to /upload . This will create a new upload object and return you a JSON representation of that object. The JSON includes the UUID, node the upload is stored on, and when it was created. Then repeatedly POST binary data to /upload/...uuid... . This binary data is blindly appended to your upload object. Do not encode the data with base64 or similar. Each call will return the new size of the object. If necessary, you can also truncate an upload object to a specified size, for example if you are unsure that a POST operation completed correctly. You do this by sending a POST to /upload/...uuid.../truncate/...desired.length... . Once your upload is complete, you convert it to an artifact by calling /artifacts/upload/...name... to convert it to an artifact. There is one final optimization to uploads, which is implemented in the python API and command line clients. If before upload you calculate a sha512 of the object to be uploaded, you can then search for that checksum with the /blob_checksums/sha512/...hash... endpoint. If a blob is returned then you don't need to actually upload and can instead pass that blob uuid (with a POST argument named blob_uuid ) instead of an upload uuid to the /artifacts/upload/...name... endpoint. See the swagger documentation for more details. Downloads Artifact downloads are implemented as fetching the data for the desired blob. You therefore must first lookup the versions for a given artifact and select a version that you wish to download. You can then fetch the data for the relevant blob by calling /blobs/...uuid.../data this call takes an optional query parameter of offset , which specifies how many bytes into the blob to start returning data from. This allows recommencing failed downloads.","title":"Artifact uploads and downloads"},{"location":"developer_guide/artifact_uploads_downloads/#artifact-uploads-and-downloads","text":"The general usage of artifact uploads and downloads is documented in the user guide . This page documents the actual API flow for uploading or downloading an artifact and is only useful to developers implementing new Shaken Fist clients.","title":"Artifact uploads and downloads"},{"location":"developer_guide/artifact_uploads_downloads/#uploads","text":"Artifact uploads normally require multiple HTTP requests in order to complete. This is because artifacts are often very large, and the REST API wants to allow you to continue an upload even if a single HTTP session fails or times out. This is implemented by creating an upload object, POSTing data to that object repeatedly, and then converting that upload object to an artifact. Upload objects which have not have data posted to them in a long time (currently 24 hours) are automatically removed. You create an upload by POST'ing to /upload . This will create a new upload object and return you a JSON representation of that object. The JSON includes the UUID, node the upload is stored on, and when it was created. Then repeatedly POST binary data to /upload/...uuid... . This binary data is blindly appended to your upload object. Do not encode the data with base64 or similar. Each call will return the new size of the object. If necessary, you can also truncate an upload object to a specified size, for example if you are unsure that a POST operation completed correctly. You do this by sending a POST to /upload/...uuid.../truncate/...desired.length... . Once your upload is complete, you convert it to an artifact by calling /artifacts/upload/...name... to convert it to an artifact. There is one final optimization to uploads, which is implemented in the python API and command line clients. If before upload you calculate a sha512 of the object to be uploaded, you can then search for that checksum with the /blob_checksums/sha512/...hash... endpoint. If a blob is returned then you don't need to actually upload and can instead pass that blob uuid (with a POST argument named blob_uuid ) instead of an upload uuid to the /artifacts/upload/...name... endpoint. See the swagger documentation for more details.","title":"Uploads"},{"location":"developer_guide/artifact_uploads_downloads/#downloads","text":"Artifact downloads are implemented as fetching the data for the desired blob. You therefore must first lookup the versions for a given artifact and select a version that you wish to download. You can then fetch the data for the relevant blob by calling /blobs/...uuid.../data this call takes an optional query parameter of offset , which specifies how many bytes into the blob to start returning data from. This allows recommencing failed downloads.","title":"Downloads"},{"location":"developer_guide/authentication/","text":"Authentication and Namespaces Shaken Fist uses JWT tokens for authentication and access control. These tokens are created with a request to the REST API and then passed as part of subsequent calls in the form of a HTTP header on the request. The tokens can expire, in which case a caller needs to re-authenticate and then retry their request. The process to create and use a token is discussed further in the Authentication section below. Shaken Fist logically divides objects into \"namespaces\". These namespaces can be thought of as tenants, although there might be other reasons to divide resources into their own buckets -- for example the Shaken Fist CI system uses a namespace to store an archive of the images used for CI runs, and that namespace is referred to by the namespaces actually running tests. The process to create a namespace is discussed in the Creating namespaces section below. Namespaces All resources (instances, networks, network interfaces, and artifacts) are assigned to a namespace. Notably, blobs are not within namespaces and more than one artifact can refer to a given blob even if those artifacts are in different namespaces. It is assumed that knowing the UUID of a given blob implies that you can access it. All requests to Shaken Fist have a namespace context. The namespace \"system\" is reserved and is used for administrative actions. Please note that the authentication configuration created by the getsf installer is for the system namespace, and if used directly will result in instances and other objects being created in that namespace. While this is supported and will function as expected, it is probably undesirable for anything other than a single user installation. By default only requests in the system namespace are able to access resources in other (foreign) namespaces. Before Shaken Fist v0.7 this behaviour was hard coded and not configurable. As of Shaken Fist v0.7, this is implemented in the form of \"trusts\", where every namespace is configured to \"trust\" the system namespace. This makes the resources visible to the system namespace. You cannot remove the trust of the system namespace from your namespaces. However, you can choose to trust additional namespaces, and this is done via the sf-client namespace trust ... series of commands and associated API calls. Authentication When the getsf installer ran, it created two authentication artifacts on the primary node which are useful to get started with Shaken Fist. First off, there is /etc/sf/sfrc , which is a file you can source in your shell to provide authentication environment variables. These environment variables can be used by Shaken Fist command line clients, Ansible modules, and the Python API client implementation itself. An example sfrc looks like this: # Command line hinting eval \"$(_SF_CLIENT_COMPLETE=bash_source sf-client)\" # Use the v3 etcd API export ETCDCTL_API=3 # Client auth export SHAKENFIST_NAMESPACE=\"system\" export SHAKENFIST_KEY=\"oisoSe7T\" export SHAKENFIST_API_URL=\"https://shakenfist/api\" The first two lines of the file enable tab completion for sf-client in a bash shell. The middle section defaults etcd to use the v3 API and can be ignored for now. The last three lines are the important authentication details: the namespace we want to use is called \"system\". our access key is \"oisoSe7T\". the URL the API exists at is \" https://shakenfist/api \" sfrc is only useful to users of Unix-like shells, so there is also a JSON form of this configuration information, which is written by getsf at /etc/sf/shakenfist.json . Here's an example: { \"namespace\": \"system\", \"key\": \"oisoSe7T\", \"apiurl\": \"https://shakenfist/api\" } The Shaken Fist command line clients, Ansible modules, and the Python API client will look for configuration in the following locations: environment variables. .shakenfist in your home directory, that is ~/.shakenfist . /etc/sf/shakenfist.json . Creating namespaces You can create your first namespace like this, assuming you are authenticated as the system namespace: sf-client namespace create new-namespace By default a new namespace has no access keys or trusts configured, and therefore is only accessible to users of the system namespace. Key management Namespaces are accessed by providing a valid \"key\" for the namespace. While keys have names, they do not have to be usernames and passwords -- my mental model is more like API access tokens in something like GitHub than usernames and passwords. I tend to create a new key for each program which is interacting with the namespace, and then give it a descriptive name. You can create a new key like this: sf-client namespace add-key namespace-name keyname key There can be more than one key for a namespace. The key name is not used as part of the authentication process, and is largely used for key management (deleting the key) and logging which access token was used in the event logs. Please note the key prefix \"_service_key\" is reserved for internal use within Shaken Fist. This usage is discussed in the Inter-node Authentication section below. Authenticating directly to the REST API The authentication endpoint /auth is used to obtain a token to authenticate future API requests. For example, I can obtain an authentication token from the REST API using curl like this: curl -X POST https://shakenfist/api/auth -d '{\"namespace\": \"system\", \"key\": \"oisoSe7T\"}' { \"access_token\": \"eyJhbG...IkpXVCJ9.eyJmc...wwQ\", \"token_type\": \"Bearer\", \"expires_in\": 900 } That is, a HTTP POST request to the /auth endpoint for the REST API (in our case hosted at https://shakenfist/api ) with a JSON body containing a dictionary of the namespace name and the key to use. In the response the access_token value of eyJhbG...IkpXVCJ9.eyJmc...wwQ is our JWT token and has been truncated in this example for readability. Authentication tokens expire after a fixed period of time (nominally 15 minutes), but you will be informed that the token as expired by receiving a 401 Unauthorized response. If that occurs, simply create a new token as above and retry your request. Subsequent requests to the REST API pass the token via an Authorization HTTP header, and should request a Content-Type of application/json . For example, to list the namespaces in our deployment we would make a curl request like this: curl -X GET https://shakenfist/api/auth/namespaces \\ -H 'Authorization: Bearer eyJhbG...IkpXVCJ9.eyJmc...wwQ' \\ -H 'Content-Type: application/json' [ { \"name\": \"adhoc\", \"state\": \"created\", \"trust\": {\"full\": [\"system\"]} }, { \"name\": \"ci\", \"state\": \"created\", \"trust\": {\"full\": [\"system\"]} }, { \"name\": \"system\", \"state\": \"created\", \"trust\": {\"full\": [\"system\"]} } ] The JSON response here has been formatted for readability. Note the word \"Bearer\" before the access token in the Authorization header. Contents of the JWT tokens JWT authentication tokens are base64 encoded parts separated by the . character. They are therefore trivial to decode. A decoded example (generated by the online decoder at https://jwt.io/ ) is: { \"alg\": \"HS256\", \"typ\": \"JWT\" } . { \"fresh\": false, \"iat\": 1669786988, \"jti\": \"906f4bfa-3218-4d07-a036-ac6b44ded67e\", \"type\": \"access\", \"sub\": [ \"system\", \"deploy\" ], \"nbf\": 1669786988, \"exp\": 1669787888, \"iss\": \"shakenfist\", \"nonce\": \"ByKNRUVBfMBoQC1Z\" } . HMACSHA256( base64UrlEncode(header) + \".\" + base64UrlEncode(payload), your-256-bit-secret ) You can see here that Shaken Fist stores the authenticated namespace system and the key used to authenticate deploy under the sub key in this token. You should not assume that the content of JWT tokens produced by Shaken Fist are opaque to users. For releases prior to v0.7, the token was blindly trusted for authentication. From v0.7 we verify that the named key still exists in the namespace before authorizing API requests. This test is performed by updating a \"nonce\" value for a given key when the key is updated. The JWT token a caller is handed includes this nonce, and if the nonce we are handed on a request does not match the current value in the database the request is rejected. Inter-node Authentication Requests between Shaken Fist nodes use the same authentication system and REST API as external API requests. When a node makes an API request to another node, the originating node will create (or reuse) a \"service key\" specific to the namespace of the original request. When a request is made from the \"system\" namespace for a resource in a different namespace, the API request is made using the foreign namespace and the foreign namespace's service key. Service keys exist in the namespace's key data structures just as other keys do, and are therefore visible when you list keys. As of v0.7, service keys expire after five minutes, and are never reused. Before v0.7 service keys were always named \"_service_key\". From v0.7 service keys have a name of the form \"_service_key[a-zA-Z]+\". Key Storage Shaken Fist stores the access keys in etcd . The keys are stored as the base64 encoding of the key post salting and hashing. The python bcrypt library is used to perform salting, hashing, and key verification.","title":"Authentication"},{"location":"developer_guide/authentication/#authentication-and-namespaces","text":"Shaken Fist uses JWT tokens for authentication and access control. These tokens are created with a request to the REST API and then passed as part of subsequent calls in the form of a HTTP header on the request. The tokens can expire, in which case a caller needs to re-authenticate and then retry their request. The process to create and use a token is discussed further in the Authentication section below. Shaken Fist logically divides objects into \"namespaces\". These namespaces can be thought of as tenants, although there might be other reasons to divide resources into their own buckets -- for example the Shaken Fist CI system uses a namespace to store an archive of the images used for CI runs, and that namespace is referred to by the namespaces actually running tests. The process to create a namespace is discussed in the Creating namespaces section below.","title":"Authentication and Namespaces"},{"location":"developer_guide/authentication/#namespaces","text":"All resources (instances, networks, network interfaces, and artifacts) are assigned to a namespace. Notably, blobs are not within namespaces and more than one artifact can refer to a given blob even if those artifacts are in different namespaces. It is assumed that knowing the UUID of a given blob implies that you can access it. All requests to Shaken Fist have a namespace context. The namespace \"system\" is reserved and is used for administrative actions. Please note that the authentication configuration created by the getsf installer is for the system namespace, and if used directly will result in instances and other objects being created in that namespace. While this is supported and will function as expected, it is probably undesirable for anything other than a single user installation. By default only requests in the system namespace are able to access resources in other (foreign) namespaces. Before Shaken Fist v0.7 this behaviour was hard coded and not configurable. As of Shaken Fist v0.7, this is implemented in the form of \"trusts\", where every namespace is configured to \"trust\" the system namespace. This makes the resources visible to the system namespace. You cannot remove the trust of the system namespace from your namespaces. However, you can choose to trust additional namespaces, and this is done via the sf-client namespace trust ... series of commands and associated API calls.","title":"Namespaces"},{"location":"developer_guide/authentication/#authentication","text":"When the getsf installer ran, it created two authentication artifacts on the primary node which are useful to get started with Shaken Fist. First off, there is /etc/sf/sfrc , which is a file you can source in your shell to provide authentication environment variables. These environment variables can be used by Shaken Fist command line clients, Ansible modules, and the Python API client implementation itself. An example sfrc looks like this: # Command line hinting eval \"$(_SF_CLIENT_COMPLETE=bash_source sf-client)\" # Use the v3 etcd API export ETCDCTL_API=3 # Client auth export SHAKENFIST_NAMESPACE=\"system\" export SHAKENFIST_KEY=\"oisoSe7T\" export SHAKENFIST_API_URL=\"https://shakenfist/api\" The first two lines of the file enable tab completion for sf-client in a bash shell. The middle section defaults etcd to use the v3 API and can be ignored for now. The last three lines are the important authentication details: the namespace we want to use is called \"system\". our access key is \"oisoSe7T\". the URL the API exists at is \" https://shakenfist/api \" sfrc is only useful to users of Unix-like shells, so there is also a JSON form of this configuration information, which is written by getsf at /etc/sf/shakenfist.json . Here's an example: { \"namespace\": \"system\", \"key\": \"oisoSe7T\", \"apiurl\": \"https://shakenfist/api\" } The Shaken Fist command line clients, Ansible modules, and the Python API client will look for configuration in the following locations: environment variables. .shakenfist in your home directory, that is ~/.shakenfist . /etc/sf/shakenfist.json .","title":"Authentication"},{"location":"developer_guide/authentication/#creating-namespaces","text":"You can create your first namespace like this, assuming you are authenticated as the system namespace: sf-client namespace create new-namespace By default a new namespace has no access keys or trusts configured, and therefore is only accessible to users of the system namespace.","title":"Creating namespaces"},{"location":"developer_guide/authentication/#key-management","text":"Namespaces are accessed by providing a valid \"key\" for the namespace. While keys have names, they do not have to be usernames and passwords -- my mental model is more like API access tokens in something like GitHub than usernames and passwords. I tend to create a new key for each program which is interacting with the namespace, and then give it a descriptive name. You can create a new key like this: sf-client namespace add-key namespace-name keyname key There can be more than one key for a namespace. The key name is not used as part of the authentication process, and is largely used for key management (deleting the key) and logging which access token was used in the event logs. Please note the key prefix \"_service_key\" is reserved for internal use within Shaken Fist. This usage is discussed in the Inter-node Authentication section below.","title":"Key management"},{"location":"developer_guide/authentication/#authenticating-directly-to-the-rest-api","text":"The authentication endpoint /auth is used to obtain a token to authenticate future API requests. For example, I can obtain an authentication token from the REST API using curl like this: curl -X POST https://shakenfist/api/auth -d '{\"namespace\": \"system\", \"key\": \"oisoSe7T\"}' { \"access_token\": \"eyJhbG...IkpXVCJ9.eyJmc...wwQ\", \"token_type\": \"Bearer\", \"expires_in\": 900 } That is, a HTTP POST request to the /auth endpoint for the REST API (in our case hosted at https://shakenfist/api ) with a JSON body containing a dictionary of the namespace name and the key to use. In the response the access_token value of eyJhbG...IkpXVCJ9.eyJmc...wwQ is our JWT token and has been truncated in this example for readability. Authentication tokens expire after a fixed period of time (nominally 15 minutes), but you will be informed that the token as expired by receiving a 401 Unauthorized response. If that occurs, simply create a new token as above and retry your request. Subsequent requests to the REST API pass the token via an Authorization HTTP header, and should request a Content-Type of application/json . For example, to list the namespaces in our deployment we would make a curl request like this: curl -X GET https://shakenfist/api/auth/namespaces \\ -H 'Authorization: Bearer eyJhbG...IkpXVCJ9.eyJmc...wwQ' \\ -H 'Content-Type: application/json' [ { \"name\": \"adhoc\", \"state\": \"created\", \"trust\": {\"full\": [\"system\"]} }, { \"name\": \"ci\", \"state\": \"created\", \"trust\": {\"full\": [\"system\"]} }, { \"name\": \"system\", \"state\": \"created\", \"trust\": {\"full\": [\"system\"]} } ] The JSON response here has been formatted for readability. Note the word \"Bearer\" before the access token in the Authorization header.","title":"Authenticating directly to the REST API"},{"location":"developer_guide/authentication/#contents-of-the-jwt-tokens","text":"JWT authentication tokens are base64 encoded parts separated by the . character. They are therefore trivial to decode. A decoded example (generated by the online decoder at https://jwt.io/ ) is: { \"alg\": \"HS256\", \"typ\": \"JWT\" } . { \"fresh\": false, \"iat\": 1669786988, \"jti\": \"906f4bfa-3218-4d07-a036-ac6b44ded67e\", \"type\": \"access\", \"sub\": [ \"system\", \"deploy\" ], \"nbf\": 1669786988, \"exp\": 1669787888, \"iss\": \"shakenfist\", \"nonce\": \"ByKNRUVBfMBoQC1Z\" } . HMACSHA256( base64UrlEncode(header) + \".\" + base64UrlEncode(payload), your-256-bit-secret ) You can see here that Shaken Fist stores the authenticated namespace system and the key used to authenticate deploy under the sub key in this token. You should not assume that the content of JWT tokens produced by Shaken Fist are opaque to users. For releases prior to v0.7, the token was blindly trusted for authentication. From v0.7 we verify that the named key still exists in the namespace before authorizing API requests. This test is performed by updating a \"nonce\" value for a given key when the key is updated. The JWT token a caller is handed includes this nonce, and if the nonce we are handed on a request does not match the current value in the database the request is rejected.","title":"Contents of the JWT tokens"},{"location":"developer_guide/authentication/#inter-node-authentication","text":"Requests between Shaken Fist nodes use the same authentication system and REST API as external API requests. When a node makes an API request to another node, the originating node will create (or reuse) a \"service key\" specific to the namespace of the original request. When a request is made from the \"system\" namespace for a resource in a different namespace, the API request is made using the foreign namespace and the foreign namespace's service key. Service keys exist in the namespace's key data structures just as other keys do, and are therefore visible when you list keys. As of v0.7, service keys expire after five minutes, and are never reused. Before v0.7 service keys were always named \"_service_key\". From v0.7 service keys have a name of the form \"_service_key[a-zA-Z]+\".","title":"Inter-node Authentication"},{"location":"developer_guide/authentication/#key-storage","text":"Shaken Fist stores the access keys in etcd . The keys are stored as the base64 encoding of the key post salting and hashing. The python bcrypt library is used to perform salting, hashing, and key verification.","title":"Key Storage"},{"location":"developer_guide/ci_api_coverage/","text":"CI API Coverage This document tracks the CI coverage for each of our public APIs. The intent to make it clear what is \"sufficiently tested\", and what needs further effort. This list is manually updated, so we'll need to show discipline in ensuring that we update it as we change APIs or CI. For this document we use the python API client as a model of what to cover, as its simpler to extract a list of calls from than the API implementation itself. We list whether we have CI which calls the API directly, CI which uses the python command line client to call the API, or both. The gold standard is both. get_instances Whilst being implied by every test tearDown(), this call is explicitly used in: TestCirros.test_cirros_boot_no_network TestCirros.test_cirros_boot_network TestPlacement.test_local_placement_works TestPlacement.test_remote_placement_works TestSystemNamespace.test_system_namespace TestUbuntu.test_ubuntu_pings delete_all_instances Not tested. get_instance All callers of await_instance_event call get_instance(). However, these more explicit tests exist as well: TestCacheImage.test_instance_invalid_image get_instance_interfaces TestMultipleNics.test_simple TestNetworking.test_virtual_networks_are_separate TestNetworking.test_overlapping_virtual_networks_are_separate TestNetworking.test_single_virtal_networks_work TestNetworking.test_specific_ip_request TestPlacement.test_local_placement_works TestPlacement.test_remote_placement_works TestStateChanges.test_lifecycle_events TestUbuntu.test_ubuntu_pings get_instance_metadata TestInstanceMetadata.test_simple set_instance_metadata_item TestInstanceMetadata.test_simple delete_instance_metadata_item Not tested. create_instance Tested extensively in most other tests, so tests are not listed here. snapshot_instance TestSnapshots.test_single_disk_snapshots TestSnapshots.test_multiple_disk_snapshots get_instance_snapshots TestSnapshots.test_single_disk_snapshots TestSnapshots.test_multiple_disk_snapshots reboot_instance TestStateChanges.test_lifecycle_events power_off_instance TestStateChanges.test_lifecycle_events power_on_instance TestStateChanges.test_lifecycle_events pause_instance TestStateChanges.test_lifecycle_events unpause_instance TestStateChanges.test_lifecycle_events delete_instance Whilst being implied by every test tearDown(), this call is explicitly used in: TestCirros.test_cirros_boot_no_network TestCirros.test_cirros_boot_network TestPlacement.test_local_placement_works TestPlacement.test_remote_placement_works TestSnapshots.test_single_disk_snapshots TestSnapshots.test_multiple_disk_snapshots TestSystemNamespace.test_system_namespace TestUbuntu.test_ubuntu_pings get_instance_events All callers of await_instance_event call get_instance(). However, more testing of this method is required. cache_image TestImages.test_cache_image get_images (Formerly get_image_meta, old name to be removed in 0.5). TestImages.test_cache_image get_image_events Not tested. get_networks Whilst being implied by every test tearDown(), this call is explicitly used in: TestSystemNamespace.test_system_namespace get_network Not tested. delete_network Whilst being implied by every test tearDown(), this call is explicitly used in: TestSystemNamespace.test_system_namespace delete_all_networks Not tested. get_network_events Not tested. allocate_network Tested extensively in most other tests, so tests are not listed here. get_network_interfaces Not tested. get_network_metadata Not tested. set_network_metadata_item Not tested. delete_network_metadata_item Not tested. get_nodes Not tested. get_interface Not tested. float_interface Not tested. defloat_interface Not tested. get_console_data TestConsoleLog.test_console_log get_namespaces As well as being tested as a side effect of most other tests, there is the following explicit test: TestAuth.test_namespaces create_namespace As well as being tested as a side effect of most other tests, there is the following explicit test: TestAuth.test_namespaces delete_namespace As well as being tested as a side effect of most other tests, there is the following explicit test: TestAuth.test_namespaces get_namespace_keynames Not tested. add_namespace_key TestAuth.test_namespaces delete_namespace_key TestAuth.test_namespaces get_namespace_metadata Not tested. set_namespace_metadata_item Not tested. delete_namespace_metadata_item Not tested. get_existing_locks Not tested. ping Tested as a side effect of many other tests, but no explicit test.","title":"CI API coverage"},{"location":"developer_guide/ci_api_coverage/#ci-api-coverage","text":"This document tracks the CI coverage for each of our public APIs. The intent to make it clear what is \"sufficiently tested\", and what needs further effort. This list is manually updated, so we'll need to show discipline in ensuring that we update it as we change APIs or CI. For this document we use the python API client as a model of what to cover, as its simpler to extract a list of calls from than the API implementation itself. We list whether we have CI which calls the API directly, CI which uses the python command line client to call the API, or both. The gold standard is both.","title":"CI API Coverage"},{"location":"developer_guide/ci_api_coverage/#get_instances","text":"Whilst being implied by every test tearDown(), this call is explicitly used in: TestCirros.test_cirros_boot_no_network TestCirros.test_cirros_boot_network TestPlacement.test_local_placement_works TestPlacement.test_remote_placement_works TestSystemNamespace.test_system_namespace TestUbuntu.test_ubuntu_pings","title":"get_instances"},{"location":"developer_guide/ci_api_coverage/#delete_all_instances","text":"Not tested.","title":"delete_all_instances"},{"location":"developer_guide/ci_api_coverage/#get_instance","text":"All callers of await_instance_event call get_instance(). However, these more explicit tests exist as well: TestCacheImage.test_instance_invalid_image","title":"get_instance"},{"location":"developer_guide/ci_api_coverage/#get_instance_interfaces","text":"TestMultipleNics.test_simple TestNetworking.test_virtual_networks_are_separate TestNetworking.test_overlapping_virtual_networks_are_separate TestNetworking.test_single_virtal_networks_work TestNetworking.test_specific_ip_request TestPlacement.test_local_placement_works TestPlacement.test_remote_placement_works TestStateChanges.test_lifecycle_events TestUbuntu.test_ubuntu_pings","title":"get_instance_interfaces"},{"location":"developer_guide/ci_api_coverage/#get_instance_metadata","text":"TestInstanceMetadata.test_simple","title":"get_instance_metadata"},{"location":"developer_guide/ci_api_coverage/#set_instance_metadata_item","text":"TestInstanceMetadata.test_simple","title":"set_instance_metadata_item"},{"location":"developer_guide/ci_api_coverage/#delete_instance_metadata_item","text":"Not tested.","title":"delete_instance_metadata_item"},{"location":"developer_guide/ci_api_coverage/#create_instance","text":"Tested extensively in most other tests, so tests are not listed here.","title":"create_instance"},{"location":"developer_guide/ci_api_coverage/#snapshot_instance","text":"TestSnapshots.test_single_disk_snapshots TestSnapshots.test_multiple_disk_snapshots","title":"snapshot_instance"},{"location":"developer_guide/ci_api_coverage/#get_instance_snapshots","text":"TestSnapshots.test_single_disk_snapshots TestSnapshots.test_multiple_disk_snapshots","title":"get_instance_snapshots"},{"location":"developer_guide/ci_api_coverage/#reboot_instance","text":"TestStateChanges.test_lifecycle_events","title":"reboot_instance"},{"location":"developer_guide/ci_api_coverage/#power_off_instance","text":"TestStateChanges.test_lifecycle_events","title":"power_off_instance"},{"location":"developer_guide/ci_api_coverage/#power_on_instance","text":"TestStateChanges.test_lifecycle_events","title":"power_on_instance"},{"location":"developer_guide/ci_api_coverage/#pause_instance","text":"TestStateChanges.test_lifecycle_events","title":"pause_instance"},{"location":"developer_guide/ci_api_coverage/#unpause_instance","text":"TestStateChanges.test_lifecycle_events","title":"unpause_instance"},{"location":"developer_guide/ci_api_coverage/#delete_instance","text":"Whilst being implied by every test tearDown(), this call is explicitly used in: TestCirros.test_cirros_boot_no_network TestCirros.test_cirros_boot_network TestPlacement.test_local_placement_works TestPlacement.test_remote_placement_works TestSnapshots.test_single_disk_snapshots TestSnapshots.test_multiple_disk_snapshots TestSystemNamespace.test_system_namespace TestUbuntu.test_ubuntu_pings","title":"delete_instance"},{"location":"developer_guide/ci_api_coverage/#get_instance_events","text":"All callers of await_instance_event call get_instance(). However, more testing of this method is required.","title":"get_instance_events"},{"location":"developer_guide/ci_api_coverage/#cache_image","text":"TestImages.test_cache_image","title":"cache_image"},{"location":"developer_guide/ci_api_coverage/#get_images","text":"(Formerly get_image_meta, old name to be removed in 0.5). TestImages.test_cache_image","title":"get_images"},{"location":"developer_guide/ci_api_coverage/#get_image_events","text":"Not tested.","title":"get_image_events"},{"location":"developer_guide/ci_api_coverage/#get_networks","text":"Whilst being implied by every test tearDown(), this call is explicitly used in: TestSystemNamespace.test_system_namespace","title":"get_networks"},{"location":"developer_guide/ci_api_coverage/#get_network","text":"Not tested.","title":"get_network"},{"location":"developer_guide/ci_api_coverage/#delete_network","text":"Whilst being implied by every test tearDown(), this call is explicitly used in: TestSystemNamespace.test_system_namespace","title":"delete_network"},{"location":"developer_guide/ci_api_coverage/#delete_all_networks","text":"Not tested.","title":"delete_all_networks"},{"location":"developer_guide/ci_api_coverage/#get_network_events","text":"Not tested.","title":"get_network_events"},{"location":"developer_guide/ci_api_coverage/#allocate_network","text":"Tested extensively in most other tests, so tests are not listed here.","title":"allocate_network"},{"location":"developer_guide/ci_api_coverage/#get_network_interfaces","text":"Not tested.","title":"get_network_interfaces"},{"location":"developer_guide/ci_api_coverage/#get_network_metadata","text":"Not tested.","title":"get_network_metadata"},{"location":"developer_guide/ci_api_coverage/#set_network_metadata_item","text":"Not tested.","title":"set_network_metadata_item"},{"location":"developer_guide/ci_api_coverage/#delete_network_metadata_item","text":"Not tested.","title":"delete_network_metadata_item"},{"location":"developer_guide/ci_api_coverage/#get_nodes","text":"Not tested.","title":"get_nodes"},{"location":"developer_guide/ci_api_coverage/#get_interface","text":"Not tested.","title":"get_interface"},{"location":"developer_guide/ci_api_coverage/#float_interface","text":"Not tested.","title":"float_interface"},{"location":"developer_guide/ci_api_coverage/#defloat_interface","text":"Not tested.","title":"defloat_interface"},{"location":"developer_guide/ci_api_coverage/#get_console_data","text":"TestConsoleLog.test_console_log","title":"get_console_data"},{"location":"developer_guide/ci_api_coverage/#get_namespaces","text":"As well as being tested as a side effect of most other tests, there is the following explicit test: TestAuth.test_namespaces","title":"get_namespaces"},{"location":"developer_guide/ci_api_coverage/#create_namespace","text":"As well as being tested as a side effect of most other tests, there is the following explicit test: TestAuth.test_namespaces","title":"create_namespace"},{"location":"developer_guide/ci_api_coverage/#delete_namespace","text":"As well as being tested as a side effect of most other tests, there is the following explicit test: TestAuth.test_namespaces","title":"delete_namespace"},{"location":"developer_guide/ci_api_coverage/#get_namespace_keynames","text":"Not tested.","title":"get_namespace_keynames"},{"location":"developer_guide/ci_api_coverage/#add_namespace_key","text":"TestAuth.test_namespaces","title":"add_namespace_key"},{"location":"developer_guide/ci_api_coverage/#delete_namespace_key","text":"TestAuth.test_namespaces","title":"delete_namespace_key"},{"location":"developer_guide/ci_api_coverage/#get_namespace_metadata","text":"Not tested.","title":"get_namespace_metadata"},{"location":"developer_guide/ci_api_coverage/#set_namespace_metadata_item","text":"Not tested.","title":"set_namespace_metadata_item"},{"location":"developer_guide/ci_api_coverage/#delete_namespace_metadata_item","text":"Not tested.","title":"delete_namespace_metadata_item"},{"location":"developer_guide/ci_api_coverage/#get_existing_locks","text":"Not tested.","title":"get_existing_locks"},{"location":"developer_guide/ci_api_coverage/#ping","text":"Tested as a side effect of many other tests, but no explicit test.","title":"ping"},{"location":"developer_guide/io_performance_tuning/","text":"IO performance tuning This page documents experiments in tuning the IO performance of Shaken Fist. It explains how we've ended up with the options we use, and what other options we considered along the way. qemu has quite a helpful guide to performance options for IO tuning at https://git.qemu.org/?p=qemu.git;a=blob;f=docs/qcow2-cache.txt , but it does not provide concrete recommendations. Before tuning First off, here are some base line performance numbers before we did any tuning. All performance tests were on a Samsung Pro 980 with libvirt / kvm / qemu. All disks were a 100GB copy on write layer on top of a 30 GB virtual backing file, on an otherwise idle machine. The virtual machine had 16 vCPU and 32 GB RAM. In the interests of the comparisons below, the untuned cluster uses writethrough caching and a cluster size of 64K: dd if=/dev/zero of=test1.img bs=1G count=50 oflag=dsync All numbers are the first run (incurs new cluster allocation cost, see later in this document), and then the average of the fastest three of five subsequent runs. Raw NVMe disk: 1,229 MB/s Before tuning: 611 MB/s, 655 MB/s (689, 589, 524, 572, 689) hdparm -Tt /dev/vda1 I have only recorded the buffered disk read value, as the other value is based on caching. I've used the same average of the fastest three from a run of five that I did above: Raw NVMe disk: 2,572 MB/s Before tuning: 1,283 MB/s (695.13, 967.22, 1142.13, 1310.75, 1396.96) Note that all experiments below are based on a single change compared to the starting state. So for example the cluster size change experiment used the default caching of writethrough. Disk cache mode qemu supports a variety of disk caching modes for disks. https://documentation.suse.com/sles/11-SP4/html/SLES-kvm4zseries/cha-qemu-cachemodes.html is a good summary of the options, which are all exposed by libvirt. Modern libvirts default to writeback, which is equivalent to using a battery backed RAID controller in a physical machine. It therefore does assume a similar level of reliability from your hypervisor nodes. Within libvirt, the caching is specified using the cache attribute to the driver element. The libvirt documentation states: The optional cache attribute controls the cache mechanism, possible values are \"default\", \"none\", \"writethrough\", \"writeback\", \"directsync\" (like \"writethrough\", but it bypasses the host page cache) and \"unsafe\" (host may cache all disk io, and sync requests from guest are ignored). Since 0.6.0, \"directsync\" since 0.9.5, \"unsafe\" since 0.9.7. So for the libvirt.tmpl domain XML template within Shaken Fist, specifying a writethrough as a caching mode would look like this: {%- for disk in disks %} {%- if disk.bus != 'nvme' %} <disk type='{{disk.source_type}}' device='{{disk.present_as}}'> <driver name='qemu' type='{{disk.type}}' cache='writethrough'/> {{disk.source}} {{disk.backing}} <target dev='{{disk.device}}' bus='{{disk.bus}}'/> </disk> {%- endif %} {%- endfor %} Some performance numbers: dd if=/dev/zero of=test1.img bs=1G count=50 oflag=dsync All numbers are the first run (incurs new cluster allocation cost, see later in this document), and then the average of the fastest three of five subsequent runs. Raw NVMe disk: 1,229 MB/s Cache none: 958 MB/s, 987 MB/s (948, 991, 1024, 866, 837) Cache writethrough: 499 MB/s, 656 MB/s (577, 735, 637, 596, 574) Cache writeback: 606 MB/s, 677 MB/s (686, 721, 589, 568, 625) Cache directsync: 626 MB/s, 823 MB/s (676, 938, 760, 771, 727) Cache unsafe: 857 MB/s, 1,012 MB/s (963, 1126, 948, 859, 840) hdparm -Tt /dev/vda1 I have only recorded the buffered disk read value, as the other value is based on caching. I've used the same average of the fastest three from a run of five that I did above: Raw NVMe disk: 2,572 MB/s Cache none: 849 MB/s (535.09, 712.36, 791.02, 859.13, 899.20) Cache writethrough: 1,352 MB/s (598.37, 927.03, 1237.42, 1300.84, 1519.23) Cache writeback: 1,589.12 MB/s (756.97, 1244.58, 1433.67, 1639.21, 1694.48) Cache directsync: 1,038.69 MB/s (914.48, 1031.09, 1065.92, 755.77, 1019.06) Cache unsafe: 1,272 MB/s (656.32, 944.18, 1122.15, 1270.44, 1424.00) What cache modes a safe in terms of data integrity? This is what https://documentation.suse.com/sles/11-SP4/html/SLES-kvm4zseries/cha-qemu-cachemodes.html has to say: cache = writethrough, cache = none, cache=directsync : These are the safest modes, and considered equally safe, given that the guest operating system is \u201cmodern and well behaved\u201d, which means that it uses flushes as needed. If you have a suspect guest, use writethough, or directsync. Note that some file systems are not compatible with cache=none or cache=directsync, as they do not support O_DIRECT, which these cache modes relies on. Specifically, I don't think that Shaken Fist should use any unsafe caching mode, which eliminates the aptly named unsafe as an option. Recommendation: we should convert to using cache mode \"none\" for instances. It provides slower read performance, but much better write performance. Cluster size qcow2 defaults to a cluster size of 64KB, and the maximum is 2MB. The value must be a power of two. The balance here is that the cluster size is the unit of allocation of disk when the disk needs to grow -- so a large cluster size will cause an image to grow larger than it might otherwise, but 2MB doesn't seem like a large overhead. However, if you are using a copy on write layer and change one byte of a fully allocated cluster, a 2MB cluster size means that 2MB must be read from the backing file, the byte changed, and then that 2MB written to the copy on write layer. That IO cost can add up depending on your workload. https://www.ibm.com/cloud/blog/how-to-tune-qemu-l2-cache-size-and-qcow2-cluster-size has a good description of how the cluster size affects cache behaviour with qcow2, as a larger cluster size also implies that you're more likely to have in-memory cache hits and avoid extra IO operations looking up caches from disk. In the worst case, a single IO can incur the actual IO operations if the cache entries required are not currently in memory. Pleasingly, cluster size is an attribute of the qcow2 file, not the hypervisor configuration. This makes it easy for us to run benchmarks against without having to tweak the hypervisor too much. You tune cluster size like this: qemu-img create -f qcow2 -o cluster_size=2M foo.qcow2 100G First off, the backing image size changes as I change the cluster size. Remember that in a hypervisor environment where the backing image is shared between VMs, the cost of increased size here is reduced by the multiple users of th backing image. It should be noted that you can have a different cluster size in the copy on write layer compared to the backing image, but I have not tested that as I want to keep the number of permutations here manageable. Some performance numbers, noting that performance will vary based on the size of the disk -- that is, very large disks would benefit from higher cluster sizes. I have selected what I think is a representative size for a Shaken Fist instance in these test runs: qemu-img convert -p -O qcow2 -o cluster_size=256K old.qcow2 new.qcow2 Cluster size 64 K: 672 MB Cluster size 128 K: 1.7 GB Cluster size 256 K: 1.7 GB Cluster size 512 K: 1.7 GB Cluster size 1,024 K: 1.7 GB Cluster size 2,048 K: 1.8 GB dd if=/dev/zero of=test1.img bs=1G count=50 oflag=dsync All numbers are the first run (incurs new cluster allocation cost), and then the average of the fastest three of five subsequent runs. Raw NVMe disk: 1,229 MB/s Cluster size 64 K: 611 MB/s, 655 MB/s (689, 589, 524, 572, 689) Cluster size 128 K: 631 MB/s, 654 MB/s (465, 701, 594, 531, 666) Cluster size 256 K: 630 MB/s, 651 MB/s (708, 667, 580, 539, 574) Cluster size 512 K: 630 MB/s, 643 MB/s (666, 680, 531, 584, 560) Cluster size 1,024 K: 632 MB/s, 631 MB/s (734, 605, 520, 555, 541) Cluster size 2,048 K: 648 MB/s, 704 MB/s (732, 731, 650, 559, 558) hdparm -Tt /dev/vda1 I have only recorded the buffered disk read value, as the other value is based on caching. I've used the same average of the fastest three from a run of five that I did above: Raw NVMe disk: 2,572 MB/s Cluster size 64 K: 1,283 MB/s (695.13, 967.22, 1142.13, 1310.75, 1396.96) Cluster size 128 K: 2,742 MB/s (2618.60, 3080.81, 2500.19, 2526.23, 1740.01) Cluster size 256 K: 3,378 MB/s (1286.86, 2167.27, 2903.52, 3399.29, 3830.55) Cluster size 512 K: 2,575 MB/s (1011.34, 1543.60, 2126.33, 2605.66, 2992.87) Cluster size 1,024 K: 2,631 MB/s (2279.93, 2329.68, 2229.67, 2622.82, 2940.58) Cluster size 2,048 K: 2,402 MB/s (899.56, 1517.90, 1992.42, 2447.05, 2767.72) Recommendation: a cluster size of 2,048K will use marginally more RAM to store the caches, but improves disk performance significantly, especially for reads. Final performance In the interests of gloating, here are our original performance numbers, compared to after tuning: dd if=/dev/zero of=test1.img bs=1G count=50 oflag=dsync All numbers are the first run (incurs new cluster allocation cost, see later in this document), and then the average of the fastest three of five subsequent runs. Raw NVMe disk: 1,229 MB/s Before tuning: 611 MB/s, 655 MB/s (689, 589, 524, 572, 689) After tuning: 992 MB/s, 991 MB/s (958, 1024, 906, 869, 838) hdparm -Tt /dev/vda1 I've used the same average of the fastest three from a run of five that I did above: Raw NVMe disk: 2,572 MB/s Before tuning: 1,283 MB/s (695.13, 967.22, 1142.13, 1310.75, 1396.96) After tuning: 1,560 MB/s (717.71, 1120.44, 1394.56, 1516.47, 1770.01)","title":"IO performance tuning"},{"location":"developer_guide/io_performance_tuning/#io-performance-tuning","text":"This page documents experiments in tuning the IO performance of Shaken Fist. It explains how we've ended up with the options we use, and what other options we considered along the way. qemu has quite a helpful guide to performance options for IO tuning at https://git.qemu.org/?p=qemu.git;a=blob;f=docs/qcow2-cache.txt , but it does not provide concrete recommendations.","title":"IO performance tuning"},{"location":"developer_guide/io_performance_tuning/#before-tuning","text":"First off, here are some base line performance numbers before we did any tuning. All performance tests were on a Samsung Pro 980 with libvirt / kvm / qemu. All disks were a 100GB copy on write layer on top of a 30 GB virtual backing file, on an otherwise idle machine. The virtual machine had 16 vCPU and 32 GB RAM. In the interests of the comparisons below, the untuned cluster uses writethrough caching and a cluster size of 64K: dd if=/dev/zero of=test1.img bs=1G count=50 oflag=dsync All numbers are the first run (incurs new cluster allocation cost, see later in this document), and then the average of the fastest three of five subsequent runs. Raw NVMe disk: 1,229 MB/s Before tuning: 611 MB/s, 655 MB/s (689, 589, 524, 572, 689) hdparm -Tt /dev/vda1 I have only recorded the buffered disk read value, as the other value is based on caching. I've used the same average of the fastest three from a run of five that I did above: Raw NVMe disk: 2,572 MB/s Before tuning: 1,283 MB/s (695.13, 967.22, 1142.13, 1310.75, 1396.96) Note that all experiments below are based on a single change compared to the starting state. So for example the cluster size change experiment used the default caching of writethrough.","title":"Before tuning"},{"location":"developer_guide/io_performance_tuning/#disk-cache-mode","text":"qemu supports a variety of disk caching modes for disks. https://documentation.suse.com/sles/11-SP4/html/SLES-kvm4zseries/cha-qemu-cachemodes.html is a good summary of the options, which are all exposed by libvirt. Modern libvirts default to writeback, which is equivalent to using a battery backed RAID controller in a physical machine. It therefore does assume a similar level of reliability from your hypervisor nodes. Within libvirt, the caching is specified using the cache attribute to the driver element. The libvirt documentation states: The optional cache attribute controls the cache mechanism, possible values are \"default\", \"none\", \"writethrough\", \"writeback\", \"directsync\" (like \"writethrough\", but it bypasses the host page cache) and \"unsafe\" (host may cache all disk io, and sync requests from guest are ignored). Since 0.6.0, \"directsync\" since 0.9.5, \"unsafe\" since 0.9.7. So for the libvirt.tmpl domain XML template within Shaken Fist, specifying a writethrough as a caching mode would look like this: {%- for disk in disks %} {%- if disk.bus != 'nvme' %} <disk type='{{disk.source_type}}' device='{{disk.present_as}}'> <driver name='qemu' type='{{disk.type}}' cache='writethrough'/> {{disk.source}} {{disk.backing}} <target dev='{{disk.device}}' bus='{{disk.bus}}'/> </disk> {%- endif %} {%- endfor %} Some performance numbers: dd if=/dev/zero of=test1.img bs=1G count=50 oflag=dsync All numbers are the first run (incurs new cluster allocation cost, see later in this document), and then the average of the fastest three of five subsequent runs. Raw NVMe disk: 1,229 MB/s Cache none: 958 MB/s, 987 MB/s (948, 991, 1024, 866, 837) Cache writethrough: 499 MB/s, 656 MB/s (577, 735, 637, 596, 574) Cache writeback: 606 MB/s, 677 MB/s (686, 721, 589, 568, 625) Cache directsync: 626 MB/s, 823 MB/s (676, 938, 760, 771, 727) Cache unsafe: 857 MB/s, 1,012 MB/s (963, 1126, 948, 859, 840) hdparm -Tt /dev/vda1 I have only recorded the buffered disk read value, as the other value is based on caching. I've used the same average of the fastest three from a run of five that I did above: Raw NVMe disk: 2,572 MB/s Cache none: 849 MB/s (535.09, 712.36, 791.02, 859.13, 899.20) Cache writethrough: 1,352 MB/s (598.37, 927.03, 1237.42, 1300.84, 1519.23) Cache writeback: 1,589.12 MB/s (756.97, 1244.58, 1433.67, 1639.21, 1694.48) Cache directsync: 1,038.69 MB/s (914.48, 1031.09, 1065.92, 755.77, 1019.06) Cache unsafe: 1,272 MB/s (656.32, 944.18, 1122.15, 1270.44, 1424.00) What cache modes a safe in terms of data integrity? This is what https://documentation.suse.com/sles/11-SP4/html/SLES-kvm4zseries/cha-qemu-cachemodes.html has to say: cache = writethrough, cache = none, cache=directsync : These are the safest modes, and considered equally safe, given that the guest operating system is \u201cmodern and well behaved\u201d, which means that it uses flushes as needed. If you have a suspect guest, use writethough, or directsync. Note that some file systems are not compatible with cache=none or cache=directsync, as they do not support O_DIRECT, which these cache modes relies on. Specifically, I don't think that Shaken Fist should use any unsafe caching mode, which eliminates the aptly named unsafe as an option. Recommendation: we should convert to using cache mode \"none\" for instances. It provides slower read performance, but much better write performance.","title":"Disk cache mode"},{"location":"developer_guide/io_performance_tuning/#cluster-size","text":"qcow2 defaults to a cluster size of 64KB, and the maximum is 2MB. The value must be a power of two. The balance here is that the cluster size is the unit of allocation of disk when the disk needs to grow -- so a large cluster size will cause an image to grow larger than it might otherwise, but 2MB doesn't seem like a large overhead. However, if you are using a copy on write layer and change one byte of a fully allocated cluster, a 2MB cluster size means that 2MB must be read from the backing file, the byte changed, and then that 2MB written to the copy on write layer. That IO cost can add up depending on your workload. https://www.ibm.com/cloud/blog/how-to-tune-qemu-l2-cache-size-and-qcow2-cluster-size has a good description of how the cluster size affects cache behaviour with qcow2, as a larger cluster size also implies that you're more likely to have in-memory cache hits and avoid extra IO operations looking up caches from disk. In the worst case, a single IO can incur the actual IO operations if the cache entries required are not currently in memory. Pleasingly, cluster size is an attribute of the qcow2 file, not the hypervisor configuration. This makes it easy for us to run benchmarks against without having to tweak the hypervisor too much. You tune cluster size like this: qemu-img create -f qcow2 -o cluster_size=2M foo.qcow2 100G First off, the backing image size changes as I change the cluster size. Remember that in a hypervisor environment where the backing image is shared between VMs, the cost of increased size here is reduced by the multiple users of th backing image. It should be noted that you can have a different cluster size in the copy on write layer compared to the backing image, but I have not tested that as I want to keep the number of permutations here manageable. Some performance numbers, noting that performance will vary based on the size of the disk -- that is, very large disks would benefit from higher cluster sizes. I have selected what I think is a representative size for a Shaken Fist instance in these test runs: qemu-img convert -p -O qcow2 -o cluster_size=256K old.qcow2 new.qcow2 Cluster size 64 K: 672 MB Cluster size 128 K: 1.7 GB Cluster size 256 K: 1.7 GB Cluster size 512 K: 1.7 GB Cluster size 1,024 K: 1.7 GB Cluster size 2,048 K: 1.8 GB dd if=/dev/zero of=test1.img bs=1G count=50 oflag=dsync All numbers are the first run (incurs new cluster allocation cost), and then the average of the fastest three of five subsequent runs. Raw NVMe disk: 1,229 MB/s Cluster size 64 K: 611 MB/s, 655 MB/s (689, 589, 524, 572, 689) Cluster size 128 K: 631 MB/s, 654 MB/s (465, 701, 594, 531, 666) Cluster size 256 K: 630 MB/s, 651 MB/s (708, 667, 580, 539, 574) Cluster size 512 K: 630 MB/s, 643 MB/s (666, 680, 531, 584, 560) Cluster size 1,024 K: 632 MB/s, 631 MB/s (734, 605, 520, 555, 541) Cluster size 2,048 K: 648 MB/s, 704 MB/s (732, 731, 650, 559, 558) hdparm -Tt /dev/vda1 I have only recorded the buffered disk read value, as the other value is based on caching. I've used the same average of the fastest three from a run of five that I did above: Raw NVMe disk: 2,572 MB/s Cluster size 64 K: 1,283 MB/s (695.13, 967.22, 1142.13, 1310.75, 1396.96) Cluster size 128 K: 2,742 MB/s (2618.60, 3080.81, 2500.19, 2526.23, 1740.01) Cluster size 256 K: 3,378 MB/s (1286.86, 2167.27, 2903.52, 3399.29, 3830.55) Cluster size 512 K: 2,575 MB/s (1011.34, 1543.60, 2126.33, 2605.66, 2992.87) Cluster size 1,024 K: 2,631 MB/s (2279.93, 2329.68, 2229.67, 2622.82, 2940.58) Cluster size 2,048 K: 2,402 MB/s (899.56, 1517.90, 1992.42, 2447.05, 2767.72) Recommendation: a cluster size of 2,048K will use marginally more RAM to store the caches, but improves disk performance significantly, especially for reads.","title":"Cluster size"},{"location":"developer_guide/io_performance_tuning/#final-performance","text":"In the interests of gloating, here are our original performance numbers, compared to after tuning: dd if=/dev/zero of=test1.img bs=1G count=50 oflag=dsync All numbers are the first run (incurs new cluster allocation cost, see later in this document), and then the average of the fastest three of five subsequent runs. Raw NVMe disk: 1,229 MB/s Before tuning: 611 MB/s, 655 MB/s (689, 589, 524, 572, 689) After tuning: 992 MB/s, 991 MB/s (958, 1024, 906, 869, 838) hdparm -Tt /dev/vda1 I've used the same average of the fastest three from a run of five that I did above: Raw NVMe disk: 2,572 MB/s Before tuning: 1,283 MB/s (695.13, 967.22, 1142.13, 1310.75, 1396.96) After tuning: 1,560 MB/s (717.71, 1120.44, 1394.56, 1516.47, 1770.01)","title":"Final performance"},{"location":"developer_guide/release_process/","text":"Shaken Fist's release process Shaken Fist is now split across a number of repositories to simplify development and usage. Unfortunately, that complicated the release process. This page documents the current release process although the reality is that only Michael can do a release right now because of the requirement to sign releases with his GPG key. Testing We only release things which have passed CI testing, and preferably have had a period running as the underlying cloud for the CI cluster as well. Sometimes in an emergency we will bend the rules for a hotfix, but we should try and avoid doing that. For reach repository to be released Checkout the repository and ensure you're in the right branch. Then just run release.sh and follow the bounching ball.","title":"Release process"},{"location":"developer_guide/release_process/#shaken-fists-release-process","text":"Shaken Fist is now split across a number of repositories to simplify development and usage. Unfortunately, that complicated the release process. This page documents the current release process although the reality is that only Michael can do a release right now because of the requirement to sign releases with his GPG key.","title":"Shaken Fist's release process"},{"location":"developer_guide/release_process/#testing","text":"We only release things which have passed CI testing, and preferably have had a period running as the underlying cloud for the CI cluster as well. Sometimes in an emergency we will bend the rules for a hotfix, but we should try and avoid doing that.","title":"Testing"},{"location":"developer_guide/release_process/#for-reach-repository-to-be-released","text":"Checkout the repository and ensure you're in the right branch. Then just run release.sh and follow the bounching ball.","title":"For reach repository to be released"},{"location":"developer_guide/standards/","text":"Concepts and Standards Ensuring a Common Language within the code base This document records the standards and common language used within the Shaken Fist software system. It should also record why the choice was made. (This is actually just notes to save our future selves from tripping over the same problems.) etcd keys Key names in etcd should be in the singular, for example /sf/namespace/ not /sf/namespaces/ note that this is different than the REST API. Memory Memory is measured in MiB in Shaken Fist. All references to memory size are stored and transmitted in MiB: Gigabytes can be too big if you want a lot of small machines. Kilobytes is just too many numbers to type. The libvirt API measures memory in KiB. Therefore, interactions with the library need to be careful to convert from MiB to KiB.","title":"Standards"},{"location":"developer_guide/standards/#concepts-and-standards","text":"","title":"Concepts and Standards"},{"location":"developer_guide/standards/#ensuring-a-common-language-within-the-code-base","text":"This document records the standards and common language used within the Shaken Fist software system. It should also record why the choice was made. (This is actually just notes to save our future selves from tripping over the same problems.)","title":"Ensuring a Common Language within the code base"},{"location":"developer_guide/standards/#etcd-keys","text":"Key names in etcd should be in the singular, for example /sf/namespace/ not /sf/namespaces/ note that this is different than the REST API.","title":"etcd keys"},{"location":"developer_guide/standards/#memory","text":"Memory is measured in MiB in Shaken Fist. All references to memory size are stored and transmitted in MiB: Gigabytes can be too big if you want a lot of small machines. Kilobytes is just too many numbers to type. The libvirt API measures memory in KiB. Therefore, interactions with the library need to be careful to convert from MiB to KiB.","title":"Memory"},{"location":"developer_guide/state_machine/","text":"Shaken Fist's State Machine Shaken Fist implements a state machine for each object type. This page documents the possible states for each object, and which transitions between states are valid. Instances Initial : this is the first state for an instance. A UUID has been allocated, a placeholder database entry created, and a request to create the instance has been queued. Preflight : the instance creation request has been dequeued and is being validated against the current state of the cluster by the scheduler. Creating : the instance is being created. Created : the instance is now running. Deleted : the instance is now deleted. Error : the instance is unable to be used. Instances marked as Deleted are deleted from the DB after sitting deleted for some period of time. Any instance may enter the Error state, which happens when something bad has happened. That process involves the instance being moved to a transition state named for the instance's previous state, so for example an instance which was Created that went into Error would transition through Created-Error. This is done because the Error transition is a queue job and happens sometime later. Instances in the Error state are not removed like those in the deleted state, as we assume a caller must acknowledge an error occured. To remove them, delete the instance in Error state. Networks Initial : first state for a network. A UUID has been allocated, database entry created, and a request to create the network on the networknode has been queued. Created : the network has been created on the networknode . Delete_Wait : the network has been scheduled for deletion. Waiting for instances on the network to be deleted. Deleted : the network is now deleted. Error : the network has encountered an error and cannot be used. Networks marked as Deleted are deleted from the DB after sitting deleted for some period of time. A network is regarded as Dead when it is in state Deleted , Delete_Wait or Error . Nodes Created : on first check in, a node is created in the \"created\" state. Stopping : the node is gracefully shutting down. Stopped : the node has gracefully shut down. Deleted : the node was manually evacuated and removed. Missing : the node has not checked in within the NODE_CHECKIN_MAXIMUM deadline. Error : the node has not check in for ten times NODE_CHECKIN_MAXIMUM, and all instances on this node have been declared to be in an error state.","title":"State machine"},{"location":"developer_guide/state_machine/#shaken-fists-state-machine","text":"Shaken Fist implements a state machine for each object type. This page documents the possible states for each object, and which transitions between states are valid.","title":"Shaken Fist's State Machine"},{"location":"developer_guide/state_machine/#instances","text":"Initial : this is the first state for an instance. A UUID has been allocated, a placeholder database entry created, and a request to create the instance has been queued. Preflight : the instance creation request has been dequeued and is being validated against the current state of the cluster by the scheduler. Creating : the instance is being created. Created : the instance is now running. Deleted : the instance is now deleted. Error : the instance is unable to be used. Instances marked as Deleted are deleted from the DB after sitting deleted for some period of time. Any instance may enter the Error state, which happens when something bad has happened. That process involves the instance being moved to a transition state named for the instance's previous state, so for example an instance which was Created that went into Error would transition through Created-Error. This is done because the Error transition is a queue job and happens sometime later. Instances in the Error state are not removed like those in the deleted state, as we assume a caller must acknowledge an error occured. To remove them, delete the instance in Error state.","title":"Instances"},{"location":"developer_guide/state_machine/#networks","text":"Initial : first state for a network. A UUID has been allocated, database entry created, and a request to create the network on the networknode has been queued. Created : the network has been created on the networknode . Delete_Wait : the network has been scheduled for deletion. Waiting for instances on the network to be deleted. Deleted : the network is now deleted. Error : the network has encountered an error and cannot be used. Networks marked as Deleted are deleted from the DB after sitting deleted for some period of time. A network is regarded as Dead when it is in state Deleted , Delete_Wait or Error .","title":"Networks"},{"location":"developer_guide/state_machine/#nodes","text":"Created : on first check in, a node is created in the \"created\" state. Stopping : the node is gracefully shutting down. Stopped : the node has gracefully shut down. Deleted : the node was manually evacuated and removed. Missing : the node has not checked in within the NODE_CHECKIN_MAXIMUM deadline. Error : the node has not check in for ten times NODE_CHECKIN_MAXIMUM, and all instances on this node have been declared to be in an error state.","title":"Nodes"},{"location":"developer_guide/updating_docs/","text":"Updating These Docs Built using MkDocs: https://www.mkdocs.org/ Theme: https://squidfunk.github.io/mkdocs-material/customization/ Setup Install mkdocs and the material theme pip install mkdocs-material Viewing Locally Start the live web-server with mkdocs serve View at http://localhost:8000 Deploying to GitHub Pages Build and deploy with mkdocs gh-deploy This will push to the gh-pages branch of the current git remote. Navigation Bar The navigation bar is configured via the mkdocs.yml file in the repository root.","title":"Updating docs"},{"location":"developer_guide/updating_docs/#updating-these-docs","text":"Built using MkDocs: https://www.mkdocs.org/ Theme: https://squidfunk.github.io/mkdocs-material/customization/","title":"Updating These Docs"},{"location":"developer_guide/updating_docs/#setup","text":"Install mkdocs and the material theme pip install mkdocs-material","title":"Setup"},{"location":"developer_guide/updating_docs/#viewing-locally","text":"Start the live web-server with mkdocs serve View at http://localhost:8000","title":"Viewing Locally"},{"location":"developer_guide/updating_docs/#deploying-to-github-pages","text":"Build and deploy with mkdocs gh-deploy This will push to the gh-pages branch of the current git remote.","title":"Deploying to GitHub Pages"},{"location":"developer_guide/updating_docs/#navigation-bar","text":"The navigation bar is configured via the mkdocs.yml file in the repository root.","title":"Navigation Bar"},{"location":"developer_guide/workflow/","text":"Development Workflow Short Lesson The majority of teams using git have a work flow that looks similar to the four well known work flows: Git Flow GitHub Flow GitLab Flow Trunk Based Development Git Development - the Shaken Fist Way The Shaken Fist developers have chosen Trunk Based Development with some minor tweaks. Branch Types Branch develop This is the development trunk and has largely replaced master in most of our respositories. All feature branches are branched from develop and merged to develop . New releases are cut from the develop branch when we decide its time to bump the major release number. The develop branch has automated nightly CI tests, and failures create GitHub issues which must be regularly triaged. That is, CI failures on develop are exceptional and should not be accepted as flakey tests or \"situation normal\". CI failures are labelled as ci-failure in GitHub issues. Feature branches Short-lived, generally a few days although sometimes much longer for complicated things. Normally only one developer. When presented to the team, it is expected to pass the linter, unit tests, and CI tests. It is normal that other team members suggest changes / improvements before merging. Branches vX.X-release Created from develop when a new major release is first cut. Patches and minor releases for that major version are then cut from this branch. Commits to this branch are cherry-picks from develop except in exceptional circumstances (for example the code to be changed no longer existing on develop ). It is not expected that many commits are made to this branch. If many commits are required to a release branch then this indicates the need for another release. \"Recent\" release branches has automated nightly CI tests, and failures create GitHub issues which must be regularly triaged. That is, CI failures on recent releases are exceptional and should not be accepted as flakey tests or \"situation normal\". CI failures are labelled as ci-failure in GitHub issues. For now, this is treated as all releases from v0.6 onwards, although that will likely change at some point. Process Bug fix branches Bug fix branches have a prefix consisting of the GitHub issue number and the word \"bug\", for example \"bug-XXX\". You commit should include the text Fixes #XXX where XXX is the GitHub issue number for the bug. It is possible to fix more than one GitHub issue in a single commit. Feature branches Feature branches should be named \"feature-branch-XXX\" where XXX is a short description of the feature. The feature branch developers should squash commits to remove WIP commits before creating a Pull Request, but it is acceptable to have a series of incremental changes building up to a complete feature in the feature branch at merge time. It is preferable that each remaining commit passes unit testing and CI, but the final state that is merged must pass unit tests and CI. Merging Commits are not squashed when merged to develop . Not squashing commits maintains history of multiple issues being solved. Pull Request related commits remain grouped and can be understood as a single merge Minimal backports Only necessary bug fixes are cherry-picked from master to an existing release branch.","title":"Workflow"},{"location":"developer_guide/workflow/#development-workflow","text":"","title":"Development Workflow"},{"location":"developer_guide/workflow/#short-lesson","text":"The majority of teams using git have a work flow that looks similar to the four well known work flows: Git Flow GitHub Flow GitLab Flow Trunk Based Development","title":"Short Lesson"},{"location":"developer_guide/workflow/#git-development-the-shaken-fist-way","text":"The Shaken Fist developers have chosen Trunk Based Development with some minor tweaks.","title":"Git Development - the Shaken Fist Way"},{"location":"developer_guide/workflow/#branch-types","text":"Branch develop This is the development trunk and has largely replaced master in most of our respositories. All feature branches are branched from develop and merged to develop . New releases are cut from the develop branch when we decide its time to bump the major release number. The develop branch has automated nightly CI tests, and failures create GitHub issues which must be regularly triaged. That is, CI failures on develop are exceptional and should not be accepted as flakey tests or \"situation normal\". CI failures are labelled as ci-failure in GitHub issues. Feature branches Short-lived, generally a few days although sometimes much longer for complicated things. Normally only one developer. When presented to the team, it is expected to pass the linter, unit tests, and CI tests. It is normal that other team members suggest changes / improvements before merging. Branches vX.X-release Created from develop when a new major release is first cut. Patches and minor releases for that major version are then cut from this branch. Commits to this branch are cherry-picks from develop except in exceptional circumstances (for example the code to be changed no longer existing on develop ). It is not expected that many commits are made to this branch. If many commits are required to a release branch then this indicates the need for another release. \"Recent\" release branches has automated nightly CI tests, and failures create GitHub issues which must be regularly triaged. That is, CI failures on recent releases are exceptional and should not be accepted as flakey tests or \"situation normal\". CI failures are labelled as ci-failure in GitHub issues. For now, this is treated as all releases from v0.6 onwards, although that will likely change at some point.","title":"Branch Types"},{"location":"developer_guide/workflow/#process","text":"","title":"Process"},{"location":"developer_guide/workflow/#bug-fix-branches","text":"Bug fix branches have a prefix consisting of the GitHub issue number and the word \"bug\", for example \"bug-XXX\". You commit should include the text Fixes #XXX where XXX is the GitHub issue number for the bug. It is possible to fix more than one GitHub issue in a single commit.","title":"Bug fix branches"},{"location":"developer_guide/workflow/#feature-branches","text":"Feature branches should be named \"feature-branch-XXX\" where XXX is a short description of the feature. The feature branch developers should squash commits to remove WIP commits before creating a Pull Request, but it is acceptable to have a series of incremental changes building up to a complete feature in the feature branch at merge time. It is preferable that each remaining commit passes unit testing and CI, but the final state that is merged must pass unit tests and CI.","title":"Feature branches"},{"location":"developer_guide/workflow/#merging","text":"Commits are not squashed when merged to develop . Not squashing commits maintains history of multiple issues being solved. Pull Request related commits remain grouped and can be understood as a single merge","title":"Merging"},{"location":"developer_guide/workflow/#minimal-backports","text":"Only necessary bug fixes are cherry-picked from master to an existing release branch.","title":"Minimal backports"},{"location":"operator_guide/artifact_checksums/","text":"Artifact checksums As of Shaken Fist v0.7, blob replics are regularly checksummed to verify that data loss has not occurred. The following events imply a checksum operation: snapshotting an NVRAM template. creation of a new blob replica by transfer of a blob from another machine in the cluster (the destination is checksummed to verify the transfer). transcode of a blob into a new format (the new format is stored as a separate blob). conversion of an upload to an artifact. The following events should imply an artifact checksum, but we found that performance suffered too much for very large blobs: download of a new blob from an external source (artifact fetch for example). snapshotting a disk. Additionally, all blob replicas are regularly checksummed and compared with what the record in etcd believes the correct value should be. These comparisons are rate limited, but should happen with a maximum frequency of CHECKSUM_VERIFICATION_FREQUENCY seconds, which defaults to every 24 hours. It is possible if you have a large number of blob replicas on a given node that the node will be unable to keep up with checksum operations. If a blob replica fails the checksum verification, CHECKSUM_ENFORCEMENT is set to True and is not in use on that node , then the replica is deleted and the cluster will re-replicate the blob as required. If the blob replica is in use, there isn't much Shaken Fist can do without disturbing running instances, so the error is logged and then ignored for now. Checksums are also used when a new version of an artifact is created. If the checksum of the previous version is the same as the checksum for the proposed new version, the proposed new version is skipped. Artifact uploads from v0.7 can also skip actual upload of the contents of the artifact if there is already a blob in the cluster with a matching checksum.","title":"Artifact checksums"},{"location":"operator_guide/artifact_checksums/#artifact-checksums","text":"As of Shaken Fist v0.7, blob replics are regularly checksummed to verify that data loss has not occurred. The following events imply a checksum operation: snapshotting an NVRAM template. creation of a new blob replica by transfer of a blob from another machine in the cluster (the destination is checksummed to verify the transfer). transcode of a blob into a new format (the new format is stored as a separate blob). conversion of an upload to an artifact. The following events should imply an artifact checksum, but we found that performance suffered too much for very large blobs: download of a new blob from an external source (artifact fetch for example). snapshotting a disk. Additionally, all blob replicas are regularly checksummed and compared with what the record in etcd believes the correct value should be. These comparisons are rate limited, but should happen with a maximum frequency of CHECKSUM_VERIFICATION_FREQUENCY seconds, which defaults to every 24 hours. It is possible if you have a large number of blob replicas on a given node that the node will be unable to keep up with checksum operations. If a blob replica fails the checksum verification, CHECKSUM_ENFORCEMENT is set to True and is not in use on that node , then the replica is deleted and the cluster will re-replicate the blob as required. If the blob replica is in use, there isn't much Shaken Fist can do without disturbing running instances, so the error is logged and then ignored for now. Checksums are also used when a new version of an artifact is created. If the checksum of the previous version is the same as the checksum for the proposed new version, the proposed new version is skipped. Artifact uploads from v0.7 can also skip actual upload of the contents of the artifact if there is already a blob in the cluster with a matching checksum.","title":"Artifact checksums"},{"location":"operator_guide/power_states/","text":"Instance power states Shaken Fist version 0.2.1 introduced power states for instances. Before this, you could power on or off an instance, or pause it, but you couldn't tell what power state the instance was actually in. That was pretty confusing and was therefore treated as a bug. The following power states are implemented: on : the instance is running off : the instance is not running paused : the instance is paused crashed : the instance is crashed according to the hypervisor. Instances in this power state will also be in an instance state of \"error\". There are additionally a set of \"transition states\" which are used to indicate that you have requested a change of state that might not yet have completed. These are: transition-to-on transition-to-off transition-to-paused We're hoping to not have to implement a transition-to-crashed state, but you never know.","title":"Power States"},{"location":"operator_guide/power_states/#instance-power-states","text":"Shaken Fist version 0.2.1 introduced power states for instances. Before this, you could power on or off an instance, or pause it, but you couldn't tell what power state the instance was actually in. That was pretty confusing and was therefore treated as a bug. The following power states are implemented: on : the instance is running off : the instance is not running paused : the instance is paused crashed : the instance is crashed according to the hypervisor. Instances in this power state will also be in an instance state of \"error\". There are additionally a set of \"transition states\" which are used to indicate that you have requested a change of state that might not yet have completed. These are: transition-to-on transition-to-off transition-to-paused We're hoping to not have to implement a transition-to-crashed state, but you never know.","title":"Instance power states"},{"location":"operator_guide/networking/overview/","text":"Networking Overview Shaken Fist networking is complicated. Its actually less complicated that OpenStack Neutron networking, and its about as simple as we can get away with, but in order to allow virtual networks to use overlapping network ranges we are forced to do some vaguely complicated things with network namespaces. This document attempts to incrementally describe how Shaken Fist networking works, so that I can remember later. VXLAN Shaken Fist networking is based on a VXLAN mesh. VXLAN is like a successor to VLANs, except that you can have 1.6 million virtual networks, it doesn't use an IP header field to divide the networks up, and it is transported inside UDP packets between the members of the mesh. Normally VXLAN meshes are implemented using multicast UDP, but that doesn't work in public clouds where Shaken Fist was born, so we instead use unicast meshes that we lovingly hand maintain. Note The virtual mesh manages itself and changes state over time. If your mesh entries do not look like mine, it does not nessesarily indicate that your networking is broken. This is a dynamic environment! Our worked examples For this document, we will assume there are three Shaken Fist nodes, named sf-1 , sf-2 , and sf-3 . Its a total coincidence that this is the default size for the installer ansible at the time of writing and the exact size of all of the production clusters we are aware of. sf-1 is configured as the \"network node\", which is just a hypervisor like every other node, except that it is also where packets to and from the virtual networks route in and out of the mesh. First, off let's start with a simple network with one instance and no DHCP or NAT .","title":"Overview"},{"location":"operator_guide/networking/overview/#networking-overview","text":"Shaken Fist networking is complicated. Its actually less complicated that OpenStack Neutron networking, and its about as simple as we can get away with, but in order to allow virtual networks to use overlapping network ranges we are forced to do some vaguely complicated things with network namespaces. This document attempts to incrementally describe how Shaken Fist networking works, so that I can remember later.","title":"Networking Overview"},{"location":"operator_guide/networking/overview/#vxlan","text":"Shaken Fist networking is based on a VXLAN mesh. VXLAN is like a successor to VLANs, except that you can have 1.6 million virtual networks, it doesn't use an IP header field to divide the networks up, and it is transported inside UDP packets between the members of the mesh. Normally VXLAN meshes are implemented using multicast UDP, but that doesn't work in public clouds where Shaken Fist was born, so we instead use unicast meshes that we lovingly hand maintain. Note The virtual mesh manages itself and changes state over time. If your mesh entries do not look like mine, it does not nessesarily indicate that your networking is broken. This is a dynamic environment!","title":"VXLAN"},{"location":"operator_guide/networking/overview/#our-worked-examples","text":"For this document, we will assume there are three Shaken Fist nodes, named sf-1 , sf-2 , and sf-3 . Its a total coincidence that this is the default size for the installer ansible at the time of writing and the exact size of all of the production clusters we are aware of. sf-1 is configured as the \"network node\", which is just a hypervisor like every other node, except that it is also where packets to and from the virtual networks route in and out of the mesh. First, off let's start with a simple network with one instance and no DHCP or NAT .","title":"Our worked examples"},{"location":"operator_guide/networking/single_instance/","text":"Networking Explained - Single Node This page is part of a series explaining how Shaken Fist networking works . You might want to check out the other pages as well. The simplest case: a virtual network with no DHCP and no NAT, and a single remote instance Let's assume you want a new virtual network with no network services. Its just two instances talking to each other. The basic flow is like this -- you create a virtual network. We allocate you a VXLAN network id (called the vxid in various places in the code): sf-1 # sf-client network create demonet 192.168.0.0/24 --no-dhcp --no-nat uuid : b41b067a-44b7-40c5-9ab8-7abdcf5fdc61 name : demonet vxlan id : 2 netblock : 192.168.0.0/24 provide dhcp : False provide nat : False floating gateway: None namespace : system state : initial Metadata: So in this case we were allocated VXLAN id 2, and have a network UUID of b41b067a-44b7-40c5-9ab8-7abdcf5fdc61. The state of the network is \"initial\" as it has not been created anywhere yet. If you wait a few seconds, you'll see it transition to a \"created\" state. You can see the new state with a show command: sf-1 # sf-client network show b41b067a-44b7-40c5-9ab8-7abdcf5fdc61 uuid : b41b067a-44b7-40c5-9ab8-7abdcf5fdc61 name : demonet vxlan id : 2 netblock : 192.168.0.0/24 provide dhcp : False provide nat : False floating gateway: None namespace : system state : created Metadata: And you can see the steps we went through to create the network in the events listing: sf-1 # sf-client network events b41b067a-44b7-40c5-9ab8-7abdcf5fdc61 +----------------------------+------+------------------------+------------+----------------------+-----------+ | timestamp | node | operation | phase | duration | message | +----------------------------+------+------------------------+------------+----------------------+-----------+ | 2020-08-01 06:49:07.753351 | sf-1 | api | create | None | None | | 2020-08-01 06:49:07.792583 | sf-1 | create vxlan interface | start | None | None | | 2020-08-01 06:49:07.819312 | sf-1 | create vxlan interface | finish | 0.025562286376953125 | None | | 2020-08-01 06:49:07.840636 | sf-1 | create vxlan bridge | start | None | None | | 2020-08-01 06:49:07.905165 | sf-1 | create vxlan bridge | finish | 0.0644679069519043 | None | | 2020-08-01 06:49:07.910409 | sf-1 | create netns | start | None | None | | 2020-08-01 06:49:07.997595 | sf-1 | create netns | finish | 0.0850062370300293 | None | | 2020-08-01 06:49:08.016509 | sf-1 | create router veth | start | None | None | | 2020-08-01 06:49:08.237167 | sf-1 | create router veth | finish | 0.2197411060333252 | None | | 2020-08-01 06:49:08.254165 | sf-1 | create physical veth | start | None | None | | 2020-08-01 06:49:08.342684 | sf-1 | create physical veth | finish | 0.08613038063049316 | None | | 2020-08-01 06:49:08.371490 | sf-1 | add mesh elements | None | None | 10.2.1.11 | | 2020-08-01 06:49:08.375682 | sf-1 | api | created | None | None | | 2020-08-01 06:49:40.664204 | sf-1 | api | get | None | None | | 2020-08-01 06:50:07.554738 | sf-1 | api | get events | None | None | +----------------------------+------+------------------------+------------+----------------------+-----------+ You can see here that the network node (sf-1) has created some network elements, and an IP (10.2.1.11) has been added to the mesh. That IP is sf-1, and its part of the network node being joined to the mesh. If we look on sf-1, we should now have a VXLAN interface, a bridge, and one end of a veth. sf-1 # ip addr show vxlan-2 287: vxlan-2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 8950 qdisc noqueue master br-vxlan-2 state UNKNOWN group default qlen 1000 link/ether de:c4:ab:3d:79:fb brd ff:ff:ff:ff:ff:ff inet6 fe80::dcc4:abff:fe3d:79fb/64 scope link valid_lft forever preferred_lft forever sf-1 # ip addr show br-vxlan-2 288: br-vxlan-2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether be:c1:1b:20:b8:f7 brd ff:ff:ff:ff:ff:ff inet6 fe80::bcc1:1bff:fe20:b8f7/64 scope link valid_lft forever preferred_lft forever sf-1 # ip addr show veth-2-o 290: veth-2-o@if289: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master br-vxlan-2 state UP group default qlen 1000 link/ether be:c1:1b:20:b8:f7 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::bcc1:1bff:fe20:b8f7/64 scope link valid_lft forever preferred_lft forever The vxlan-2 interface is the VXLAN mesh, and the br-vxlan-2 bridge is how VMs and veths will connect to the mesh on this local machine. Its important to note that MTU matters here. The MTU for the mesh network is 1500 bytes, and most client VMs will default to that as well. Therefore the underlying network needs to have a MTU greater than that. We default to an MTU of 9000 bytes in our installs, but 1550 would in fact be sufficient in this case. You can see this in the MTU for vxlan-2, which is our 9000 byte underlying MTU, with 50 bytes deducted for the VXLAN encapsulation. We can also ask the mesh for its current state: sf-1 # bridge fdb show brport vxlan-2 de:c4:ab:3d:79:fb master br-vxlan-2 permanent de:c4:ab:3d:79:fb vlan 1 master br-vxlan-2 permanent 00:00:00:00:00:00 dst 10.2.1.11 self permanent de:c4:ab:3d:79:fb dst 127.0.0.1 self be:c1:1b:20:b8:f7 dst 127.0.0.1 self e2:2d:32:6d:95:70 dst 127.0.0.1 self The current members of the mesh are: de:c4:ab:3d:79:fb: this is the mac address for vxlan-2. 00:00:00:00:00:00 dst 10.2.1.11: this is a mesh entry for the node with IP 10.2.1.11 (sf-1) be:c1:1b:20:b8:f7: this is the outside mac address of a veth between br-vxlan-2 and a network namespace on sf-1 e2:2d:32:6d:95:70: is the inside mac address of the veth between br-vxlan-2 and the network namespace What is this network namespace? Well, Shaken Fist needs to create a network namespace to contain routing, NAT, and DHCP for the virtual network. It's actually not strictly required in this simplest case, but we always create it. It is named for the UUID of the virtual network: sf-1 # ip netns exec b41b067a-44b7-40c5-9ab8-7abdcf5fdc61 ip addr list 1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 289: veth-2-i@if290: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether e2:2d:32:6d:95:70 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 192.168.0.1/24 scope global veth-2-i valid_lft forever preferred_lft forever inet6 fe80::e02d:32ff:fe6d:9570/64 scope link valid_lft forever preferred_lft forever 291: phy-2-i@if292: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether fe:18:8e:f8:d1:5c brd ff:ff:ff:ff:ff:ff link-netnsid 0 The veth between the VXLAN mesh and this namespace is named veth-2-i (the interface inside the network namespace) and veth-2-o (the interface outside the network namespace). There is another veth named phy-2-i and phy-2-o, which is a link between the namespace and the outside world, but we'll talk about that more when we enable NAT. For those who are new to veths, think of them like patch cables -- so what we have here is a VXLAN mesh, which is patched into a network namespace, which is in turn patched into the outside world. We also do some things with iptables, especially around NAT. Here's the current state of iptables in the network namespace: sf-1 # ip netns exec b41b067a-44b7-40c5-9ab8-7abdcf5fdc61 iptables -L -t nat Chain PREROUTING (policy ACCEPT) target prot opt source destination Chain INPUT (policy ACCEPT) target prot opt source destination Chain OUTPUT (policy ACCEPT) target prot opt source destination Chain POSTROUTING (policy ACCEPT) target prot opt source destination That's empty for now because we're not doing any NAT yet, but watch this space. Next let's now start an instance on sf-2. This instance can't use DHCP to get an address because we have that disabled for this network. sf-1 # sf-client instance create inst-on-sf-2 1 1024 -d 20@ubuntu:18.04 -n b41b067a-44b7-40c5-9ab8-7abdcf5fdc61 -p sf-2 uuid : d53b345f-9d5d-493c-a1a3-7e7d6513cfa2 name : inst-on-sf-2 namespace : system cpus : 1 memory : 1024 disk spec : type=disk bus=None size=20 base=ubuntu:18.04 video : model=cirrus memory=16384 node : sf-2 power state : on state : created console port: 48420 vdi port : 32122 ssh key : None user data : None Metadata: Interfaces: uuid : db372968-1c74-4033-8423-8a698e9d4900 network : b41b067a-44b7-40c5-9ab8-7abdcf5fdc61 macaddr : 00:00:00:34:8d:90 order : 0 ipv4 : 192.168.0.3 floating: None model : virtio You can see that our instance (inst-on-sf-2) has been placed on sf-2 because we asked nicely (the -p is a placement option to the command), and has been allocated an IP (192.168.0.3). The virtual network still allocates IPs, even if DHCP is disabled. It has also been allocated a MAC address (00:00:00:34:8d:90). What is the state of the mesh on the network node now? sf-1 # bridge fdb show brport vxlan-2 de:c4:ab:3d:79:fb master br-vxlan-2 permanent de:c4:ab:3d:79:fb vlan 1 master br-vxlan-2 permanent 00:00:00:00:00:00 dst 10.2.1.11 self permanent 0a:ce:c4:f7:cb:dc dst 10.2.1.12 self de:c4:ab:3d:79:fb dst 127.0.0.1 self be:c1:1b:20:b8:f7 dst 127.0.0.1 self 00:00:00:34:8d:90 dst 10.2.1.12 self e2:2d:32:6d:95:70 dst 127.0.0.1 self The following entries there are new: 0a:ce:c4:f7:cb:dc dst 10.2.1.12 self 00:00:00:34:8d:90 dst 10.2.1.12 self These new entries: Add our new instance to the mesh (00:00:00:34:8d:90) And add vxlan-2 on sf-2 to the mesh (0a:ce:c4:f7:cb:dc) To repeat some commands from above but on sf-2, we now have two new network interfaces over there: sf-2 # ip addr list vxlan-2 127: vxlan-2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 8950 qdisc noqueue master br-vxlan-2 state UNKNOWN group default qlen 1000 link/ether 0a:ce:c4:f7:cb:dc brd ff:ff:ff:ff:ff:ff inet6 fe80::8ce:c4ff:fef7:cbdc/64 scope link valid_lft forever preferred_lft forever sf-2 # ip addr show br-vxlan-2 128: br-vxlan-2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 8950 qdisc noqueue state UP group default qlen 1000 link/ether 0a:ce:c4:f7:cb:dc brd ff:ff:ff:ff:ff:ff inet6 fe80::8ce:c4ff:fef7:cbdc/64 scope link valid_lft forever preferred_lft forever And the mesh looks like this: sf-2 # bridge fdb show brport vxlan-2 0a:ce:c4:f7:cb:dc master br-vxlan-2 permanent 0a:ce:c4:f7:cb:dc vlan 1 master br-vxlan-2 permanent 00:00:00:00:00:00 dst 10.2.1.11 self permanent 00:00:00:00:00:00 dst 10.2.1.12 self permanent 0a:ce:c4:f7:cb:dc dst 127.0.0.1 self 00:00:00:34:8d:90 dst 127.0.0.1 self e2:2d:32:6d:95:70 dst 10.2.1.11 self There is no network namespace on sf-2: sf-2 # ls /var/run/netns ls: cannot access '/var/run/netns': No such file or directory If we grep the console log of the instance on sf-2, we can see what cloud-init thought about the networking: sf-1 # grep ci-info /srv/shakenfist/instances/d53b345f-9d5d-493c-a1a3-7e7d6513cfa2/console.log [ 9.851378] cloud-init[643]: ci-info: +++++++++++++++++++++++++++++++++++++Net device info++++++++++++++++++++++++++++++++++++++ [ 9.856401] cloud-init[643]: ci-info: +--------+------+---------------------------+---------------+--------+-------------------+ [ 9.858628] cloud-init[643]: ci-info: | Device | Up | Address | Mask | Scope | Hw-Address | [ 9.862566] cloud-init[643]: ci-info: +--------+------+---------------------------+---------------+--------+-------------------+ [ 9.873611] cloud-init[643]: ci-info: | eth0 | True | 192.168.0.3 | 255.255.255.0 | global | 00:00:00:34:8d:90 | [ 9.884575] cloud-init[643]: ci-info: | eth0 | True | fe80::200:ff:fe34:8d90/64 | . | link | 00:00:00:34:8d:90 | [ 9.896315] cloud-init[643]: ci-info: | lo | True | 127.0.0.1 | 255.0.0.0 | host | . | [ 9.899272] cloud-init[643]: ci-info: | lo | True | ::1/128 | . | host | . | [ 9.904106] cloud-init[643]: ci-info: +--------+------+---------------------------+---------------+--------+-------------------+ [ 9.907816] cloud-init[643]: ci-info: +++++++++++++++++++++++++++++Route IPv4 info+++++++++++++++++++++++++++++ [ 9.912118] cloud-init[643]: ci-info: +-------+-------------+-------------+---------------+-----------+-------+ [ 9.913889] cloud-init[643]: ci-info: | Route | Destination | Gateway | Genmask | Interface | Flags | [ 9.917853] cloud-init[643]: ci-info: +-------+-------------+-------------+---------------+-----------+-------+ [ 9.920102] cloud-init[643]: ci-info: | 0 | 0.0.0.0 | 192.168.0.1 | 0.0.0.0 | eth0 | UG | [ 9.926243] cloud-init[643]: ci-info: | 1 | 192.168.0.0 | 0.0.0.0 | 255.255.255.0 | eth0 | U | [ 9.929182] cloud-init[643]: ci-info: +-------+-------------+-------------+---------------+-----------+-------+ [ 9.935249] cloud-init[643]: ci-info: +++++++++++++++++++Route IPv6 info+++++++++++++++++++ [ 9.942774] cloud-init[643]: ci-info: +-------+-------------+---------+-----------+-------+ [ 9.956377] cloud-init[643]: ci-info: | Route | Destination | Gateway | Interface | Flags | [ 9.959651] cloud-init[643]: ci-info: +-------+-------------+---------+-----------+-------+ [ 9.965082] cloud-init[643]: ci-info: | 1 | fe80::/64 | :: | eth0 | U | [ 9.968116] cloud-init[643]: ci-info: | 3 | local | :: | eth0 | U | [ 9.970423] cloud-init[643]: ci-info: | 4 | ff00::/8 | :: | eth0 | U | [ 9.974926] cloud-init[643]: ci-info: +-------+-------------+---------+-----------+-------+ ci-info: no authorized SSH keys fingerprints found for user ubuntu. [ 69.743452] cloud-init[970]: ci-info: no authorized SSH keys fingerprints found for user ubuntu. Here you can see that instance has an interface named eth0, which has the IP address that Shaken Fist allocated earlier. How did it get an IP address without DHCP? Well, Shaken Fist always attaches a config drive to the instance, and this contains a JSON file with the IP address in it. cloud-init running on boot of Ubuntu has used this to configure the interface. Before we poke more at this instance, let's start another instance on sf-3 so we can do some more testing...","title":"Single Instance"},{"location":"operator_guide/networking/single_instance/#networking-explained-single-node","text":"This page is part of a series explaining how Shaken Fist networking works . You might want to check out the other pages as well.","title":"Networking Explained - Single Node"},{"location":"operator_guide/networking/single_instance/#the-simplest-case-a-virtual-network-with-no-dhcp-and-no-nat-and-a-single-remote-instance","text":"Let's assume you want a new virtual network with no network services. Its just two instances talking to each other. The basic flow is like this -- you create a virtual network. We allocate you a VXLAN network id (called the vxid in various places in the code): sf-1 # sf-client network create demonet 192.168.0.0/24 --no-dhcp --no-nat uuid : b41b067a-44b7-40c5-9ab8-7abdcf5fdc61 name : demonet vxlan id : 2 netblock : 192.168.0.0/24 provide dhcp : False provide nat : False floating gateway: None namespace : system state : initial Metadata: So in this case we were allocated VXLAN id 2, and have a network UUID of b41b067a-44b7-40c5-9ab8-7abdcf5fdc61. The state of the network is \"initial\" as it has not been created anywhere yet. If you wait a few seconds, you'll see it transition to a \"created\" state. You can see the new state with a show command: sf-1 # sf-client network show b41b067a-44b7-40c5-9ab8-7abdcf5fdc61 uuid : b41b067a-44b7-40c5-9ab8-7abdcf5fdc61 name : demonet vxlan id : 2 netblock : 192.168.0.0/24 provide dhcp : False provide nat : False floating gateway: None namespace : system state : created Metadata: And you can see the steps we went through to create the network in the events listing: sf-1 # sf-client network events b41b067a-44b7-40c5-9ab8-7abdcf5fdc61 +----------------------------+------+------------------------+------------+----------------------+-----------+ | timestamp | node | operation | phase | duration | message | +----------------------------+------+------------------------+------------+----------------------+-----------+ | 2020-08-01 06:49:07.753351 | sf-1 | api | create | None | None | | 2020-08-01 06:49:07.792583 | sf-1 | create vxlan interface | start | None | None | | 2020-08-01 06:49:07.819312 | sf-1 | create vxlan interface | finish | 0.025562286376953125 | None | | 2020-08-01 06:49:07.840636 | sf-1 | create vxlan bridge | start | None | None | | 2020-08-01 06:49:07.905165 | sf-1 | create vxlan bridge | finish | 0.0644679069519043 | None | | 2020-08-01 06:49:07.910409 | sf-1 | create netns | start | None | None | | 2020-08-01 06:49:07.997595 | sf-1 | create netns | finish | 0.0850062370300293 | None | | 2020-08-01 06:49:08.016509 | sf-1 | create router veth | start | None | None | | 2020-08-01 06:49:08.237167 | sf-1 | create router veth | finish | 0.2197411060333252 | None | | 2020-08-01 06:49:08.254165 | sf-1 | create physical veth | start | None | None | | 2020-08-01 06:49:08.342684 | sf-1 | create physical veth | finish | 0.08613038063049316 | None | | 2020-08-01 06:49:08.371490 | sf-1 | add mesh elements | None | None | 10.2.1.11 | | 2020-08-01 06:49:08.375682 | sf-1 | api | created | None | None | | 2020-08-01 06:49:40.664204 | sf-1 | api | get | None | None | | 2020-08-01 06:50:07.554738 | sf-1 | api | get events | None | None | +----------------------------+------+------------------------+------------+----------------------+-----------+ You can see here that the network node (sf-1) has created some network elements, and an IP (10.2.1.11) has been added to the mesh. That IP is sf-1, and its part of the network node being joined to the mesh. If we look on sf-1, we should now have a VXLAN interface, a bridge, and one end of a veth. sf-1 # ip addr show vxlan-2 287: vxlan-2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 8950 qdisc noqueue master br-vxlan-2 state UNKNOWN group default qlen 1000 link/ether de:c4:ab:3d:79:fb brd ff:ff:ff:ff:ff:ff inet6 fe80::dcc4:abff:fe3d:79fb/64 scope link valid_lft forever preferred_lft forever sf-1 # ip addr show br-vxlan-2 288: br-vxlan-2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether be:c1:1b:20:b8:f7 brd ff:ff:ff:ff:ff:ff inet6 fe80::bcc1:1bff:fe20:b8f7/64 scope link valid_lft forever preferred_lft forever sf-1 # ip addr show veth-2-o 290: veth-2-o@if289: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master br-vxlan-2 state UP group default qlen 1000 link/ether be:c1:1b:20:b8:f7 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::bcc1:1bff:fe20:b8f7/64 scope link valid_lft forever preferred_lft forever The vxlan-2 interface is the VXLAN mesh, and the br-vxlan-2 bridge is how VMs and veths will connect to the mesh on this local machine. Its important to note that MTU matters here. The MTU for the mesh network is 1500 bytes, and most client VMs will default to that as well. Therefore the underlying network needs to have a MTU greater than that. We default to an MTU of 9000 bytes in our installs, but 1550 would in fact be sufficient in this case. You can see this in the MTU for vxlan-2, which is our 9000 byte underlying MTU, with 50 bytes deducted for the VXLAN encapsulation. We can also ask the mesh for its current state: sf-1 # bridge fdb show brport vxlan-2 de:c4:ab:3d:79:fb master br-vxlan-2 permanent de:c4:ab:3d:79:fb vlan 1 master br-vxlan-2 permanent 00:00:00:00:00:00 dst 10.2.1.11 self permanent de:c4:ab:3d:79:fb dst 127.0.0.1 self be:c1:1b:20:b8:f7 dst 127.0.0.1 self e2:2d:32:6d:95:70 dst 127.0.0.1 self The current members of the mesh are: de:c4:ab:3d:79:fb: this is the mac address for vxlan-2. 00:00:00:00:00:00 dst 10.2.1.11: this is a mesh entry for the node with IP 10.2.1.11 (sf-1) be:c1:1b:20:b8:f7: this is the outside mac address of a veth between br-vxlan-2 and a network namespace on sf-1 e2:2d:32:6d:95:70: is the inside mac address of the veth between br-vxlan-2 and the network namespace What is this network namespace? Well, Shaken Fist needs to create a network namespace to contain routing, NAT, and DHCP for the virtual network. It's actually not strictly required in this simplest case, but we always create it. It is named for the UUID of the virtual network: sf-1 # ip netns exec b41b067a-44b7-40c5-9ab8-7abdcf5fdc61 ip addr list 1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 289: veth-2-i@if290: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether e2:2d:32:6d:95:70 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 192.168.0.1/24 scope global veth-2-i valid_lft forever preferred_lft forever inet6 fe80::e02d:32ff:fe6d:9570/64 scope link valid_lft forever preferred_lft forever 291: phy-2-i@if292: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether fe:18:8e:f8:d1:5c brd ff:ff:ff:ff:ff:ff link-netnsid 0 The veth between the VXLAN mesh and this namespace is named veth-2-i (the interface inside the network namespace) and veth-2-o (the interface outside the network namespace). There is another veth named phy-2-i and phy-2-o, which is a link between the namespace and the outside world, but we'll talk about that more when we enable NAT. For those who are new to veths, think of them like patch cables -- so what we have here is a VXLAN mesh, which is patched into a network namespace, which is in turn patched into the outside world. We also do some things with iptables, especially around NAT. Here's the current state of iptables in the network namespace: sf-1 # ip netns exec b41b067a-44b7-40c5-9ab8-7abdcf5fdc61 iptables -L -t nat Chain PREROUTING (policy ACCEPT) target prot opt source destination Chain INPUT (policy ACCEPT) target prot opt source destination Chain OUTPUT (policy ACCEPT) target prot opt source destination Chain POSTROUTING (policy ACCEPT) target prot opt source destination That's empty for now because we're not doing any NAT yet, but watch this space. Next let's now start an instance on sf-2. This instance can't use DHCP to get an address because we have that disabled for this network. sf-1 # sf-client instance create inst-on-sf-2 1 1024 -d 20@ubuntu:18.04 -n b41b067a-44b7-40c5-9ab8-7abdcf5fdc61 -p sf-2 uuid : d53b345f-9d5d-493c-a1a3-7e7d6513cfa2 name : inst-on-sf-2 namespace : system cpus : 1 memory : 1024 disk spec : type=disk bus=None size=20 base=ubuntu:18.04 video : model=cirrus memory=16384 node : sf-2 power state : on state : created console port: 48420 vdi port : 32122 ssh key : None user data : None Metadata: Interfaces: uuid : db372968-1c74-4033-8423-8a698e9d4900 network : b41b067a-44b7-40c5-9ab8-7abdcf5fdc61 macaddr : 00:00:00:34:8d:90 order : 0 ipv4 : 192.168.0.3 floating: None model : virtio You can see that our instance (inst-on-sf-2) has been placed on sf-2 because we asked nicely (the -p is a placement option to the command), and has been allocated an IP (192.168.0.3). The virtual network still allocates IPs, even if DHCP is disabled. It has also been allocated a MAC address (00:00:00:34:8d:90). What is the state of the mesh on the network node now? sf-1 # bridge fdb show brport vxlan-2 de:c4:ab:3d:79:fb master br-vxlan-2 permanent de:c4:ab:3d:79:fb vlan 1 master br-vxlan-2 permanent 00:00:00:00:00:00 dst 10.2.1.11 self permanent 0a:ce:c4:f7:cb:dc dst 10.2.1.12 self de:c4:ab:3d:79:fb dst 127.0.0.1 self be:c1:1b:20:b8:f7 dst 127.0.0.1 self 00:00:00:34:8d:90 dst 10.2.1.12 self e2:2d:32:6d:95:70 dst 127.0.0.1 self The following entries there are new: 0a:ce:c4:f7:cb:dc dst 10.2.1.12 self 00:00:00:34:8d:90 dst 10.2.1.12 self These new entries: Add our new instance to the mesh (00:00:00:34:8d:90) And add vxlan-2 on sf-2 to the mesh (0a:ce:c4:f7:cb:dc) To repeat some commands from above but on sf-2, we now have two new network interfaces over there: sf-2 # ip addr list vxlan-2 127: vxlan-2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 8950 qdisc noqueue master br-vxlan-2 state UNKNOWN group default qlen 1000 link/ether 0a:ce:c4:f7:cb:dc brd ff:ff:ff:ff:ff:ff inet6 fe80::8ce:c4ff:fef7:cbdc/64 scope link valid_lft forever preferred_lft forever sf-2 # ip addr show br-vxlan-2 128: br-vxlan-2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 8950 qdisc noqueue state UP group default qlen 1000 link/ether 0a:ce:c4:f7:cb:dc brd ff:ff:ff:ff:ff:ff inet6 fe80::8ce:c4ff:fef7:cbdc/64 scope link valid_lft forever preferred_lft forever And the mesh looks like this: sf-2 # bridge fdb show brport vxlan-2 0a:ce:c4:f7:cb:dc master br-vxlan-2 permanent 0a:ce:c4:f7:cb:dc vlan 1 master br-vxlan-2 permanent 00:00:00:00:00:00 dst 10.2.1.11 self permanent 00:00:00:00:00:00 dst 10.2.1.12 self permanent 0a:ce:c4:f7:cb:dc dst 127.0.0.1 self 00:00:00:34:8d:90 dst 127.0.0.1 self e2:2d:32:6d:95:70 dst 10.2.1.11 self There is no network namespace on sf-2: sf-2 # ls /var/run/netns ls: cannot access '/var/run/netns': No such file or directory If we grep the console log of the instance on sf-2, we can see what cloud-init thought about the networking: sf-1 # grep ci-info /srv/shakenfist/instances/d53b345f-9d5d-493c-a1a3-7e7d6513cfa2/console.log [ 9.851378] cloud-init[643]: ci-info: +++++++++++++++++++++++++++++++++++++Net device info++++++++++++++++++++++++++++++++++++++ [ 9.856401] cloud-init[643]: ci-info: +--------+------+---------------------------+---------------+--------+-------------------+ [ 9.858628] cloud-init[643]: ci-info: | Device | Up | Address | Mask | Scope | Hw-Address | [ 9.862566] cloud-init[643]: ci-info: +--------+------+---------------------------+---------------+--------+-------------------+ [ 9.873611] cloud-init[643]: ci-info: | eth0 | True | 192.168.0.3 | 255.255.255.0 | global | 00:00:00:34:8d:90 | [ 9.884575] cloud-init[643]: ci-info: | eth0 | True | fe80::200:ff:fe34:8d90/64 | . | link | 00:00:00:34:8d:90 | [ 9.896315] cloud-init[643]: ci-info: | lo | True | 127.0.0.1 | 255.0.0.0 | host | . | [ 9.899272] cloud-init[643]: ci-info: | lo | True | ::1/128 | . | host | . | [ 9.904106] cloud-init[643]: ci-info: +--------+------+---------------------------+---------------+--------+-------------------+ [ 9.907816] cloud-init[643]: ci-info: +++++++++++++++++++++++++++++Route IPv4 info+++++++++++++++++++++++++++++ [ 9.912118] cloud-init[643]: ci-info: +-------+-------------+-------------+---------------+-----------+-------+ [ 9.913889] cloud-init[643]: ci-info: | Route | Destination | Gateway | Genmask | Interface | Flags | [ 9.917853] cloud-init[643]: ci-info: +-------+-------------+-------------+---------------+-----------+-------+ [ 9.920102] cloud-init[643]: ci-info: | 0 | 0.0.0.0 | 192.168.0.1 | 0.0.0.0 | eth0 | UG | [ 9.926243] cloud-init[643]: ci-info: | 1 | 192.168.0.0 | 0.0.0.0 | 255.255.255.0 | eth0 | U | [ 9.929182] cloud-init[643]: ci-info: +-------+-------------+-------------+---------------+-----------+-------+ [ 9.935249] cloud-init[643]: ci-info: +++++++++++++++++++Route IPv6 info+++++++++++++++++++ [ 9.942774] cloud-init[643]: ci-info: +-------+-------------+---------+-----------+-------+ [ 9.956377] cloud-init[643]: ci-info: | Route | Destination | Gateway | Interface | Flags | [ 9.959651] cloud-init[643]: ci-info: +-------+-------------+---------+-----------+-------+ [ 9.965082] cloud-init[643]: ci-info: | 1 | fe80::/64 | :: | eth0 | U | [ 9.968116] cloud-init[643]: ci-info: | 3 | local | :: | eth0 | U | [ 9.970423] cloud-init[643]: ci-info: | 4 | ff00::/8 | :: | eth0 | U | [ 9.974926] cloud-init[643]: ci-info: +-------+-------------+---------+-----------+-------+ ci-info: no authorized SSH keys fingerprints found for user ubuntu. [ 69.743452] cloud-init[970]: ci-info: no authorized SSH keys fingerprints found for user ubuntu. Here you can see that instance has an interface named eth0, which has the IP address that Shaken Fist allocated earlier. How did it get an IP address without DHCP? Well, Shaken Fist always attaches a config drive to the instance, and this contains a JSON file with the IP address in it. cloud-init running on boot of Ubuntu has used this to configure the interface. Before we poke more at this instance, let's start another instance on sf-3 so we can do some more testing...","title":"The simplest case: a virtual network with no DHCP and no NAT, and a single remote instance"},{"location":"user_guide/artifacts/","text":"Artifact creation Shaken Fist uses artifacts as disk templates for new instances. You therefore need to have a least one artifact before you can create your first instance, although there is a shorthand notation to create that artifact during instance creation. The following artifact types exist: images : these are generally downloaded from the Internet, although they can also be created via an upload mechanism (see the artifact uploads section below for a detailed introduction to uploading images to the cluster). snapshots : these are created by taking a copy of the disk(s) of a running instance. labels : labels are a bit like symbolic links, although they still have versioning like other forms of artifact. Behind the scenes, artifacts are references to blobs. You can think of them as symlinks if you'd like. All types of artifact support versioning. This is implemented by having each artifact have a list of blobs. You can see this in the output of a sf-client artifact show ... command: $ sf-client artifact show 3420f4ac-529a-4b34-b8d8-c05a838b9e0c uuid : 3420f4ac-529a-4b34-b8d8-c05a838b9e0c namespace : ci-images type : label state : created source url : sf://label/ci-images/debian-11 current version blob uuid: cc6a6a96-8182-474a-ab31-45f1f9310b44 number of versions : 3 maximum versions : 3 shared : False Versions: 4 : blob f6090574-321c-4dec-b381-0caf001eeba5 is 2964.1MB 5 : blob 77b9032a-0d3e-4cc2-bb73-1730ad3c3cb0 is 2960.1MB 6 : blob cc6a6a96-8182-474a-ab31-45f1f9310b44 is 2950.4MB in use by instances 78d566f1-c664-46d9-aa46-cf717aa63743 Here we can see a few things: The artifact is a label in the ci-images namespace. There is a source URL, which is how you would reference this artifact when starting an instance. There are three versions of the artifact currently stored ( number of versions ), which is the maximum ( maximum versions ). However, there have been six versions of this artifact ever (as shown by the indices of the versions being 4, 5, and 6). The most recent version is currently in use by an instance. Creating an image artifact Image artifacts are created by specifying the URL of an image to download. This can be either in the form of an explicit request to cache a URL with a sf-client artifact cache command, or implied by specifying the URL of the image in the specification for an instance. There is also a shorthand form of image URLs where you are using an image from https://images.shakenfist.com -- in that case you can use urls like debian:11 so specify the latest version of a Debian 11 base image. The valid shorthands are those listed in the top level directory listing of that site. At the time of writing this is: centos (shorthand for centos:8-stream) centos:7 centos:8-stream debian (shorthand for debian:11) debian:10 debian:11 ubuntu (shorthand for ubuntu:20.04) ubuntu:18.04 ubuntu:20.04 These images are updated nightly by an automated job from https://github.com/shakenfist/images . Whenever you specify a URL for an image (either a new cache command or at instance start), the URL is checked. If the image has changed a new version is downloaded, otherwise the already cached version is used. You can also create an image artifact by uploading it, but that's complicated enough that its covered separately in artifact uploads and downloads . Creating a snapshot artifact These are created by the sf-client instance snapshot command. You can specify which disk to snapshot on a multi-disk instance with the --device flag. Repeated snapshots of the same instance will result in multiple versions of the one artifact being created. Note that there is an artifact per device snapshotted, so a single snapshot of a multi-disk instance will create multiple artifacts. Creating label artifacts So what's a label? Well downloading new versions of images automatically is great, but what if I want to ensure the version from two weeks ago that I tested is the one I use? Or what if I want to refer to my favourite instance snapshot by something more convenient than a snapshot URL like sf://instance/78d566f1-c664-46d9-aa46-cf717aa63743/vda ? Well, labels are the answer to those questions. Labels are artifacts where you specify what the new version should be. So in the download example you'd test an image version and when you decide that its right, you'd add that version's blob UUID to your label of known tested versions. Blobs are reference counted, so even if the image artifact ages out a version, having that version referred to in a label artifact protects it from deletion. An example of labelling a known good version of an artifact would be something like this: $ sf-client artifact show 3420f4ac-529a-4b34-b8d8-c05a838b9e0c uuid : 3420f4ac-529a-4b34-b8d8-c05a838b9e0c namespace : ci-images type : label state : created source url : sf://label/ci-images/debian-11 current version blob uuid: cc6a6a96-8182-474a-ab31-45f1f9310b44 number of versions : 3 maximum versions : 3 shared : False Versions: 4 : blob f6090574-321c-4dec-b381-0caf001eeba5 is 2964.1MB 5 : blob 77b9032a-0d3e-4cc2-bb73-1730ad3c3cb0 is 2960.1MB 6 : blob cc6a6a96-8182-474a-ab31-45f1f9310b44 is 2950.4MB in use by instances 78d566f1-c664-46d9-aa46-cf717aa63743 ...test version 6 with blob UUID cc6a6a96-8182-474a-ab31-45f1f9310b44... $ sf-client label update my-tested-thing cc6a6a96-8182-474a-ab31-45f1f9310b44 If the label my-tested-thing does not exist, it will be created the first time you update it. Controlling the number of versions You can also control the number of versions stored by an artifact with the sf-client artifact max-versions command. Blob replication You can control the number of copies of a given blob are stored in the cluster as well. This protects against machine or disk failures causing data loss. The default number of replicas is 2, but this is not configurable per-blob. It is configured with the BLOB_REPLICATION_FACTOR configuration variable. Artifact uploads and downloads Artifacts may also be uploaded and downloaded. This means you can extract a snapshot from your cluster for offline backup (or movement to another cloud), or upload an image built with a tool like Hashicorp Packer. To upload an artifact, use the sf-client artifact upload command. To download an artifact, use the sf-client artifact download command.","title":"Artifacts"},{"location":"user_guide/artifacts/#artifact-creation","text":"Shaken Fist uses artifacts as disk templates for new instances. You therefore need to have a least one artifact before you can create your first instance, although there is a shorthand notation to create that artifact during instance creation. The following artifact types exist: images : these are generally downloaded from the Internet, although they can also be created via an upload mechanism (see the artifact uploads section below for a detailed introduction to uploading images to the cluster). snapshots : these are created by taking a copy of the disk(s) of a running instance. labels : labels are a bit like symbolic links, although they still have versioning like other forms of artifact. Behind the scenes, artifacts are references to blobs. You can think of them as symlinks if you'd like. All types of artifact support versioning. This is implemented by having each artifact have a list of blobs. You can see this in the output of a sf-client artifact show ... command: $ sf-client artifact show 3420f4ac-529a-4b34-b8d8-c05a838b9e0c uuid : 3420f4ac-529a-4b34-b8d8-c05a838b9e0c namespace : ci-images type : label state : created source url : sf://label/ci-images/debian-11 current version blob uuid: cc6a6a96-8182-474a-ab31-45f1f9310b44 number of versions : 3 maximum versions : 3 shared : False Versions: 4 : blob f6090574-321c-4dec-b381-0caf001eeba5 is 2964.1MB 5 : blob 77b9032a-0d3e-4cc2-bb73-1730ad3c3cb0 is 2960.1MB 6 : blob cc6a6a96-8182-474a-ab31-45f1f9310b44 is 2950.4MB in use by instances 78d566f1-c664-46d9-aa46-cf717aa63743 Here we can see a few things: The artifact is a label in the ci-images namespace. There is a source URL, which is how you would reference this artifact when starting an instance. There are three versions of the artifact currently stored ( number of versions ), which is the maximum ( maximum versions ). However, there have been six versions of this artifact ever (as shown by the indices of the versions being 4, 5, and 6). The most recent version is currently in use by an instance.","title":"Artifact creation"},{"location":"user_guide/artifacts/#creating-an-image-artifact","text":"Image artifacts are created by specifying the URL of an image to download. This can be either in the form of an explicit request to cache a URL with a sf-client artifact cache command, or implied by specifying the URL of the image in the specification for an instance. There is also a shorthand form of image URLs where you are using an image from https://images.shakenfist.com -- in that case you can use urls like debian:11 so specify the latest version of a Debian 11 base image. The valid shorthands are those listed in the top level directory listing of that site. At the time of writing this is: centos (shorthand for centos:8-stream) centos:7 centos:8-stream debian (shorthand for debian:11) debian:10 debian:11 ubuntu (shorthand for ubuntu:20.04) ubuntu:18.04 ubuntu:20.04 These images are updated nightly by an automated job from https://github.com/shakenfist/images . Whenever you specify a URL for an image (either a new cache command or at instance start), the URL is checked. If the image has changed a new version is downloaded, otherwise the already cached version is used. You can also create an image artifact by uploading it, but that's complicated enough that its covered separately in artifact uploads and downloads .","title":"Creating an image artifact"},{"location":"user_guide/artifacts/#creating-a-snapshot-artifact","text":"These are created by the sf-client instance snapshot command. You can specify which disk to snapshot on a multi-disk instance with the --device flag. Repeated snapshots of the same instance will result in multiple versions of the one artifact being created. Note that there is an artifact per device snapshotted, so a single snapshot of a multi-disk instance will create multiple artifacts.","title":"Creating a snapshot artifact"},{"location":"user_guide/artifacts/#creating-label-artifacts","text":"So what's a label? Well downloading new versions of images automatically is great, but what if I want to ensure the version from two weeks ago that I tested is the one I use? Or what if I want to refer to my favourite instance snapshot by something more convenient than a snapshot URL like sf://instance/78d566f1-c664-46d9-aa46-cf717aa63743/vda ? Well, labels are the answer to those questions. Labels are artifacts where you specify what the new version should be. So in the download example you'd test an image version and when you decide that its right, you'd add that version's blob UUID to your label of known tested versions. Blobs are reference counted, so even if the image artifact ages out a version, having that version referred to in a label artifact protects it from deletion. An example of labelling a known good version of an artifact would be something like this: $ sf-client artifact show 3420f4ac-529a-4b34-b8d8-c05a838b9e0c uuid : 3420f4ac-529a-4b34-b8d8-c05a838b9e0c namespace : ci-images type : label state : created source url : sf://label/ci-images/debian-11 current version blob uuid: cc6a6a96-8182-474a-ab31-45f1f9310b44 number of versions : 3 maximum versions : 3 shared : False Versions: 4 : blob f6090574-321c-4dec-b381-0caf001eeba5 is 2964.1MB 5 : blob 77b9032a-0d3e-4cc2-bb73-1730ad3c3cb0 is 2960.1MB 6 : blob cc6a6a96-8182-474a-ab31-45f1f9310b44 is 2950.4MB in use by instances 78d566f1-c664-46d9-aa46-cf717aa63743 ...test version 6 with blob UUID cc6a6a96-8182-474a-ab31-45f1f9310b44... $ sf-client label update my-tested-thing cc6a6a96-8182-474a-ab31-45f1f9310b44 If the label my-tested-thing does not exist, it will be created the first time you update it.","title":"Creating label artifacts"},{"location":"user_guide/artifacts/#controlling-the-number-of-versions","text":"You can also control the number of versions stored by an artifact with the sf-client artifact max-versions command.","title":"Controlling the number of versions"},{"location":"user_guide/artifacts/#blob-replication","text":"You can control the number of copies of a given blob are stored in the cluster as well. This protects against machine or disk failures causing data loss. The default number of replicas is 2, but this is not configurable per-blob. It is configured with the BLOB_REPLICATION_FACTOR configuration variable.","title":"Blob replication"},{"location":"user_guide/artifacts/#artifact-uploads-and-downloads","text":"Artifacts may also be uploaded and downloaded. This means you can extract a snapshot from your cluster for offline backup (or movement to another cloud), or upload an image built with a tool like Hashicorp Packer. To upload an artifact, use the sf-client artifact upload command. To download an artifact, use the sf-client artifact download command.","title":"Artifact uploads and downloads"},{"location":"user_guide/events/","text":"Events Events are Shaken Fist's audit log mechanism. Many operations, ranging from creation and subsequent use of an authentication token, to any change in the data for an object, will result in an event being created in the audit log for the relevant object. Importantly, regular \"usage events\" are also emitted, which we expect would form the basis for a consumption based billing system. Events may be requested using the sf-client ...object... events command, for example sf-client artifact events ...uuid... will return the events for the relevant artifact. The schema for events is still in flux, so be careful implementing automated systems which consume events. This will remain true until we are more confident that all relevant audit lock events are being collected. At that point we will standardize and stabilize the interface.","title":"Events"},{"location":"user_guide/events/#events","text":"Events are Shaken Fist's audit log mechanism. Many operations, ranging from creation and subsequent use of an authentication token, to any change in the data for an object, will result in an event being created in the audit log for the relevant object. Importantly, regular \"usage events\" are also emitted, which we expect would form the basis for a consumption based billing system. Events may be requested using the sf-client ...object... events command, for example sf-client artifact events ...uuid... will return the events for the relevant artifact. The schema for events is still in flux, so be careful implementing automated systems which consume events. This will remain true until we are more confident that all relevant audit lock events are being collected. At that point we will standardize and stabilize the interface.","title":"Events"},{"location":"user_guide/installation/","text":"Installing Shaken Fist The purpose of this guide is to walk you through a Shaken Fist installation. Shaken Fist will work just fine on a single machine, although its also happy to run on clusters of machines. We'll discuss the general guidance for install options as we go. Shaken Fist only supports Ubuntu 20.04, Debian 10, and Debian 11, so if you're running on localhost that implies that you must be running a recent Ubuntu or Debian on your development machine. Note as well that the deployer installs software and changes the configuration of your networking, so be careful when running it on machines you are fond of. This documentation was most recently tested against Debian 11, in November 2021. Bug reports are welcome if you have any issues, and may be filed at https://github.com/shakenfist/shakenfist/issues Each machine in the cluster should match this description: Have virtualization extensions enabled in the BIOS. Have jumbo frames enabled on the switch for the \"mesh interface\" for installations of more than one machine. Shaken Fist can optionally run internal traffic such as etcd and virtual network meshes on a separate interface to traffic egressing the cluster. Whichever interface you specify as being used for virtual network mesh traffic must have jumbo frames enabled for the virtual networks to function correctly. Have at least 1 gigabit connectivity on the \"mesh interface\". This is a requirement of etcd. Have a cloudadmin account setup with passwordless sudo, and a ssh key in its authorized_keys file. This is an ansible requirement, although the exact username is configurable in the SSH_USER variable. We now have a fancy helper to help you install your first localhost cluster, so let's give that a go: curl https://raw.githubusercontent.com/shakenfist/shakenfist/develop/deploy/getsf -o getsf chmod ugo+rx getsf sudo ./getsf This script will then walk you through the installation steps, asking questions as you go. The script leaves you with an installer configuration at /root/sf-deploy , which is the basis for later upgrades and cluster expansions. You can script the answers to getsf by setting environment variables. For example: export GETSF_FLOATING_BLOCK=192.168.10.0/24 export GETSF_DEPLOY_NAME=bonkerslab export GETSF_RELEASE=pre-release export GETSF_NODES=localhost export GETSF_WARNING=yes sudo --preserve-env ./getsf Sample configuration for a multi-node deploy However, if you're performing a multinode deploy, the process is currently much more manual. The intent is to expand getsf to handle multiple nodes, but that work is not yet complete. For now, you'll need to write a configuration file a bit like this on the primary node: export ADMIN_PASSWORD=engeeF1o export FLOATING_IP_BLOCK=\"192.168.10.0/24\" export DEPLOY_NAME=\"bonkerslab\" export SSH_USER=\"cloudadmin\" export SSH_KEY_FILENAME=\"/root/.ssh/id_rsa\" export KSM_ENABLED=1 # Topology is in JSON read -r -d '' TOPOLOGY <<'EOF' [ { \"name\": \"sf-primary\", \"node_egress_ip\": \"192.168.1.50\", \"node_egress_nic\": \"enp0s31f6\", \"node_mesh_ip\": \"192.168.21.50\", \"node_mesh_nic\": \"enp0s31f6:1\", \"primary_node\": true, \"api_url\": \"https://...your...install...here.com/api\" }, { \"name\": \"sf-1\", \"node_egress_ip\": \"192.168.1.51\", \"node_egress_nic\": \"enp5s0\", \"node_mesh_ip\": \"192.168.21.51\", \"node_mesh_nic\": \"eno1\", \"etcd_master\": true, \"network_node\": true, \"hypervisor\": true }, { \"name\": \"sf-2\", \"node_egress_ip\": \"192.168.1.52\", \"node_egress_nic\": \"enp5s0\", \"node_mesh_ip\": \"192.168.21.52\", \"node_mesh_nic\": \"eno1\", \"etcd_master\": true, \"hypervisor\": true }, { \"name\": \"sf-3\", \"node_egress_ip\": \"192.168.1.53\", \"node_egress_nic\": \"enp5s0\", \"node_mesh_ip\": \"192.168.21.53\", \"node_mesh_nic\": \"eno1\", \"etcd_master\": true, \"hypervisor\": true }, ] EOF export TOPOLOGY /srv/shakenfist/venv/share/shakenfist/installer/install Notes for multi-node installations Not every node needs to be an etcd_master. I'd select three in most situations. One node must be marked as the primary node, and one must be marked as the network node. It is not currently supported having more than one of each of those node types. The primary node runs an apache load balancer across the API servers in the cluster, and therefore needs to be accessable to your users on HTTP and HTTPS. The network node is the ingress and egress point for all virtual networks, and is where floating IPs live, so it needs to be setup as the gateway fro your floating IP block. Some of the considerations here can be subtle. Please reach out if you need a hand. For a node complicated installation, sf-deploy might like this: #!/bin/bash export ADMIN_PASSWORD=engeeF1o export FLOATING_IP_BLOCK=\"192.168.10.0/24\" export DEPLOY_NAME=\"bonkerslab\" export SSH_USER=\"cloudadmin\" export SSH_KEY_FILENAME=\"/root/.ssh/id_rsa\" export KSM_ENABLED=1 # Topology is in JSON read -r -d '' TOPOLOGY <<'EOF' [ { \"name\": \"sf-primary\", \"node_egress_ip\": \"192.168.1.50\", \"node_egress_nic\": \"enp0s31f6\", \"node_mesh_ip\": \"192.168.21.50\", \"node_mesh_nic\": \"enp0s31f6:1\", \"primary_node\": true, \"api_url\": \"https://...your...install...here.com/api\" }, { \"name\": \"sf-1\", \"node_egress_ip\": \"192.168.1.51\", \"node_egress_nic\": \"enp5s0\", \"node_mesh_ip\": \"192.168.21.51\", \"node_mesh_nic\": \"eno1\", \"etcd_master\": true, \"network_node\": true, \"hypervisor\": true }, { \"name\": \"sf-2\", \"node_egress_ip\": \"192.168.1.52\", \"node_egress_nic\": \"enp5s0\", \"node_mesh_ip\": \"192.168.21.52\", \"node_mesh_nic\": \"eno1\", \"etcd_master\": true, \"hypervisor\": true }, { \"name\": \"sf-3\", \"node_egress_ip\": \"192.168.1.53\", \"node_egress_nic\": \"enp5s0\", \"node_mesh_ip\": \"192.168.21.53\", \"node_mesh_nic\": \"eno1\", \"etcd_master\": true, \"hypervisor\": true }, ] EOF export TOPOLOGY /srv/shakenfist/venv/share/shakenfist/installer/install Your first instance Before you can start your first instance you'll need to authenticate to Shaken Fist, and create a network. Shaken Fist's python api client (as used by the command line client) looks for authentication details in the following locations: Command line flags Environment variables (prefixed with SHAKENFIST_ ) ~/.shakenfist , a JSON formatted configuration file /etc/sf/shakenfist.json , the same file as above, but global By default the installer creates /etc/sf/sfrc , which sets the required environment variables to authenticate. It is customized per installation, setting the following variables: SHAKENFIST_NAMESPACE , the namespace to create resources in SHAKENFIST_KEY , an authentication key for that namespace SHAKENFIST_API_URL , a URL to the Shaken Fist API server Before interacting with Shaken Fist, we need to source the rc file. . /etc/sf/sfrc Instances must be launched attached to a network. Create your first network: sf-client network create mynet 192.168.42.0/24 You can get help for the command line client by running `sf-client --help . The above command creates a new network called \"mynet\", with the IP block 192.168.42.0/24. You will receive some descriptive output back: $ sf-client network create mynet 192.168.42.0/24 uuid : 16baa325-5adf-473f-8e7a-75710a822d45 name : mynet vxlan id : 2 netblock : 192.168.42.0/24 provide dhcp : True provide nat : True floating gateway: None namespace : system state : initial Metadata: The UUID is important, as that is how we will refer to the network elsewhere. Let's now create a simple first instance (you'll need to change this to use your actual network UUID): $ sf-client instance create myvm 1 1024 -d 8@cirros -n 16baa325-5adf-473f-8e7a-75710a822d45 uuid : c6c4ba94-ed34-497d-8964-c223489dee3e name : myvm namespace : system cpus : 1 memory : 1024 disk spec : type=disk bus=None size=8 base=cirros video : model=cirrus memory=16384 node : marvin power state : on state : created console port: 31839 vdi port : 34442 ssh key : None user data : None Metadata: Interfaces: uuid : e56b3c7b-8056-4645-b5b5-1779721ff21d network : 16baa325-5adf-473f-8e7a-75710a822d45 macaddr : ae:15:4d:9c:d8:c0 order : 0 ipv4 : 192.168.42.76 floating: None model : virtio Probably the easiest way to interact with this instance is to connect to its console port, which is the serial console of the instance over telnet. In the case above, that is available on port 31829 on localhost (my laptop is called marvin). Other caveats The installer will also enforce the following sanity checks: That KVM will operate on your machines. This is generally fine unless you're using virtual machines at which point nested virtualization needs to be enabled. That your network interface MTU is greater than 2,000 bytes. This is required because the VXLAN mesh our virtual networks use add overhead to packets and a standard MTU of 1500 bytes for the physical network will result in packets being fragmented too frequently on the virtual networks. You can set a higher MTU if you desire, I generally select 9,000 bytes. Deployment variables Option Description ADMIN_PASSWORD The admin password for the cloud once installed DNS_SERVER The DNS server to configure instances with via DHCP. Defaults to 8.8.8.8 HTTP_PROXY A URL for a HTTP proxy to use for image downloads. For example http://localhost:3128 INCLUDE_TRACEBACKS Whether to include tracebacks in server 500 errors. Never set this to true in production! FLOATING_IP_BLOCK The IP range to use for the floating network KSM_ENABLED Set to 1 to enable KSM, 0 to disable DEPLOY_NAME The name of the deployment to use as an external label for prometheus TOPOLOGY The topology of the cluster, as described above SSH_KEY_FILENAME The path to a ssh private key file to use for authentication. It is assumed that the public key is at ${SSH_KEY_FILENAME}.pub . SSH_USER The username to ssh as.","title":"Installation"},{"location":"user_guide/installation/#installing-shaken-fist","text":"The purpose of this guide is to walk you through a Shaken Fist installation. Shaken Fist will work just fine on a single machine, although its also happy to run on clusters of machines. We'll discuss the general guidance for install options as we go. Shaken Fist only supports Ubuntu 20.04, Debian 10, and Debian 11, so if you're running on localhost that implies that you must be running a recent Ubuntu or Debian on your development machine. Note as well that the deployer installs software and changes the configuration of your networking, so be careful when running it on machines you are fond of. This documentation was most recently tested against Debian 11, in November 2021. Bug reports are welcome if you have any issues, and may be filed at https://github.com/shakenfist/shakenfist/issues Each machine in the cluster should match this description: Have virtualization extensions enabled in the BIOS. Have jumbo frames enabled on the switch for the \"mesh interface\" for installations of more than one machine. Shaken Fist can optionally run internal traffic such as etcd and virtual network meshes on a separate interface to traffic egressing the cluster. Whichever interface you specify as being used for virtual network mesh traffic must have jumbo frames enabled for the virtual networks to function correctly. Have at least 1 gigabit connectivity on the \"mesh interface\". This is a requirement of etcd. Have a cloudadmin account setup with passwordless sudo, and a ssh key in its authorized_keys file. This is an ansible requirement, although the exact username is configurable in the SSH_USER variable. We now have a fancy helper to help you install your first localhost cluster, so let's give that a go: curl https://raw.githubusercontent.com/shakenfist/shakenfist/develop/deploy/getsf -o getsf chmod ugo+rx getsf sudo ./getsf This script will then walk you through the installation steps, asking questions as you go. The script leaves you with an installer configuration at /root/sf-deploy , which is the basis for later upgrades and cluster expansions. You can script the answers to getsf by setting environment variables. For example: export GETSF_FLOATING_BLOCK=192.168.10.0/24 export GETSF_DEPLOY_NAME=bonkerslab export GETSF_RELEASE=pre-release export GETSF_NODES=localhost export GETSF_WARNING=yes sudo --preserve-env ./getsf","title":"Installing Shaken Fist"},{"location":"user_guide/installation/#sample-configuration-for-a-multi-node-deploy","text":"However, if you're performing a multinode deploy, the process is currently much more manual. The intent is to expand getsf to handle multiple nodes, but that work is not yet complete. For now, you'll need to write a configuration file a bit like this on the primary node: export ADMIN_PASSWORD=engeeF1o export FLOATING_IP_BLOCK=\"192.168.10.0/24\" export DEPLOY_NAME=\"bonkerslab\" export SSH_USER=\"cloudadmin\" export SSH_KEY_FILENAME=\"/root/.ssh/id_rsa\" export KSM_ENABLED=1 # Topology is in JSON read -r -d '' TOPOLOGY <<'EOF' [ { \"name\": \"sf-primary\", \"node_egress_ip\": \"192.168.1.50\", \"node_egress_nic\": \"enp0s31f6\", \"node_mesh_ip\": \"192.168.21.50\", \"node_mesh_nic\": \"enp0s31f6:1\", \"primary_node\": true, \"api_url\": \"https://...your...install...here.com/api\" }, { \"name\": \"sf-1\", \"node_egress_ip\": \"192.168.1.51\", \"node_egress_nic\": \"enp5s0\", \"node_mesh_ip\": \"192.168.21.51\", \"node_mesh_nic\": \"eno1\", \"etcd_master\": true, \"network_node\": true, \"hypervisor\": true }, { \"name\": \"sf-2\", \"node_egress_ip\": \"192.168.1.52\", \"node_egress_nic\": \"enp5s0\", \"node_mesh_ip\": \"192.168.21.52\", \"node_mesh_nic\": \"eno1\", \"etcd_master\": true, \"hypervisor\": true }, { \"name\": \"sf-3\", \"node_egress_ip\": \"192.168.1.53\", \"node_egress_nic\": \"enp5s0\", \"node_mesh_ip\": \"192.168.21.53\", \"node_mesh_nic\": \"eno1\", \"etcd_master\": true, \"hypervisor\": true }, ] EOF export TOPOLOGY /srv/shakenfist/venv/share/shakenfist/installer/install","title":"Sample configuration for a multi-node deploy"},{"location":"user_guide/installation/#notes-for-multi-node-installations","text":"Not every node needs to be an etcd_master. I'd select three in most situations. One node must be marked as the primary node, and one must be marked as the network node. It is not currently supported having more than one of each of those node types. The primary node runs an apache load balancer across the API servers in the cluster, and therefore needs to be accessable to your users on HTTP and HTTPS. The network node is the ingress and egress point for all virtual networks, and is where floating IPs live, so it needs to be setup as the gateway fro your floating IP block. Some of the considerations here can be subtle. Please reach out if you need a hand. For a node complicated installation, sf-deploy might like this: #!/bin/bash export ADMIN_PASSWORD=engeeF1o export FLOATING_IP_BLOCK=\"192.168.10.0/24\" export DEPLOY_NAME=\"bonkerslab\" export SSH_USER=\"cloudadmin\" export SSH_KEY_FILENAME=\"/root/.ssh/id_rsa\" export KSM_ENABLED=1 # Topology is in JSON read -r -d '' TOPOLOGY <<'EOF' [ { \"name\": \"sf-primary\", \"node_egress_ip\": \"192.168.1.50\", \"node_egress_nic\": \"enp0s31f6\", \"node_mesh_ip\": \"192.168.21.50\", \"node_mesh_nic\": \"enp0s31f6:1\", \"primary_node\": true, \"api_url\": \"https://...your...install...here.com/api\" }, { \"name\": \"sf-1\", \"node_egress_ip\": \"192.168.1.51\", \"node_egress_nic\": \"enp5s0\", \"node_mesh_ip\": \"192.168.21.51\", \"node_mesh_nic\": \"eno1\", \"etcd_master\": true, \"network_node\": true, \"hypervisor\": true }, { \"name\": \"sf-2\", \"node_egress_ip\": \"192.168.1.52\", \"node_egress_nic\": \"enp5s0\", \"node_mesh_ip\": \"192.168.21.52\", \"node_mesh_nic\": \"eno1\", \"etcd_master\": true, \"hypervisor\": true }, { \"name\": \"sf-3\", \"node_egress_ip\": \"192.168.1.53\", \"node_egress_nic\": \"enp5s0\", \"node_mesh_ip\": \"192.168.21.53\", \"node_mesh_nic\": \"eno1\", \"etcd_master\": true, \"hypervisor\": true }, ] EOF export TOPOLOGY /srv/shakenfist/venv/share/shakenfist/installer/install","title":"Notes for multi-node installations"},{"location":"user_guide/installation/#your-first-instance","text":"Before you can start your first instance you'll need to authenticate to Shaken Fist, and create a network. Shaken Fist's python api client (as used by the command line client) looks for authentication details in the following locations: Command line flags Environment variables (prefixed with SHAKENFIST_ ) ~/.shakenfist , a JSON formatted configuration file /etc/sf/shakenfist.json , the same file as above, but global By default the installer creates /etc/sf/sfrc , which sets the required environment variables to authenticate. It is customized per installation, setting the following variables: SHAKENFIST_NAMESPACE , the namespace to create resources in SHAKENFIST_KEY , an authentication key for that namespace SHAKENFIST_API_URL , a URL to the Shaken Fist API server Before interacting with Shaken Fist, we need to source the rc file. . /etc/sf/sfrc Instances must be launched attached to a network. Create your first network: sf-client network create mynet 192.168.42.0/24 You can get help for the command line client by running `sf-client --help . The above command creates a new network called \"mynet\", with the IP block 192.168.42.0/24. You will receive some descriptive output back: $ sf-client network create mynet 192.168.42.0/24 uuid : 16baa325-5adf-473f-8e7a-75710a822d45 name : mynet vxlan id : 2 netblock : 192.168.42.0/24 provide dhcp : True provide nat : True floating gateway: None namespace : system state : initial Metadata: The UUID is important, as that is how we will refer to the network elsewhere. Let's now create a simple first instance (you'll need to change this to use your actual network UUID): $ sf-client instance create myvm 1 1024 -d 8@cirros -n 16baa325-5adf-473f-8e7a-75710a822d45 uuid : c6c4ba94-ed34-497d-8964-c223489dee3e name : myvm namespace : system cpus : 1 memory : 1024 disk spec : type=disk bus=None size=8 base=cirros video : model=cirrus memory=16384 node : marvin power state : on state : created console port: 31839 vdi port : 34442 ssh key : None user data : None Metadata: Interfaces: uuid : e56b3c7b-8056-4645-b5b5-1779721ff21d network : 16baa325-5adf-473f-8e7a-75710a822d45 macaddr : ae:15:4d:9c:d8:c0 order : 0 ipv4 : 192.168.42.76 floating: None model : virtio Probably the easiest way to interact with this instance is to connect to its console port, which is the serial console of the instance over telnet. In the case above, that is available on port 31829 on localhost (my laptop is called marvin).","title":"Your first instance"},{"location":"user_guide/installation/#other-caveats","text":"The installer will also enforce the following sanity checks: That KVM will operate on your machines. This is generally fine unless you're using virtual machines at which point nested virtualization needs to be enabled. That your network interface MTU is greater than 2,000 bytes. This is required because the VXLAN mesh our virtual networks use add overhead to packets and a standard MTU of 1500 bytes for the physical network will result in packets being fragmented too frequently on the virtual networks. You can set a higher MTU if you desire, I generally select 9,000 bytes.","title":"Other caveats"},{"location":"user_guide/installation/#deployment-variables","text":"Option Description ADMIN_PASSWORD The admin password for the cloud once installed DNS_SERVER The DNS server to configure instances with via DHCP. Defaults to 8.8.8.8 HTTP_PROXY A URL for a HTTP proxy to use for image downloads. For example http://localhost:3128 INCLUDE_TRACEBACKS Whether to include tracebacks in server 500 errors. Never set this to true in production! FLOATING_IP_BLOCK The IP range to use for the floating network KSM_ENABLED Set to 1 to enable KSM, 0 to disable DEPLOY_NAME The name of the deployment to use as an external label for prometheus TOPOLOGY The topology of the cluster, as described above SSH_KEY_FILENAME The path to a ssh private key file to use for authentication. It is assumed that the public key is at ${SSH_KEY_FILENAME}.pub . SSH_USER The username to ssh as.","title":"Deployment variables"},{"location":"user_guide/objects/","text":"Objects Everything that you interact with in Shaken Fist is an object. Objects are almost always referred to by a UUID (specifically a version 4 UUID) as a string. The exceptions are: node s; namespace s; and key s within a namespace. In general an object is referred to in the API or on the command line \"by reference\", which means you can either pass the object's name or its UUID to the command. So for example if we had an instance with the UUID 0a38d51e-2f72-4848-80fb-03031978633b named \"mikal\", then we could run either of the commands below to the same effect: sf-client instance show 0a38d51e-2f72-4848-80fb-03031978633b sf-client instance show mikal In the case where you refer to an object by name, a lookup occurs of all objects visible to you (those in your namespace, and namespaces that trust your namespace). Additionally, shared artifacts are included if you're using an artifact command. It is possible that the name you're using isn't unique. For example there might be two instances named \"mikal\" with different UUIDs. In that case, you will get an error indicating that there was more than one object which matched, and you'll need to use a UUID to refer to the object.","title":"Objects"},{"location":"user_guide/objects/#objects","text":"Everything that you interact with in Shaken Fist is an object. Objects are almost always referred to by a UUID (specifically a version 4 UUID) as a string. The exceptions are: node s; namespace s; and key s within a namespace. In general an object is referred to in the API or on the command line \"by reference\", which means you can either pass the object's name or its UUID to the command. So for example if we had an instance with the UUID 0a38d51e-2f72-4848-80fb-03031978633b named \"mikal\", then we could run either of the commands below to the same effect: sf-client instance show 0a38d51e-2f72-4848-80fb-03031978633b sf-client instance show mikal In the case where you refer to an object by name, a lookup occurs of all objects visible to you (those in your namespace, and namespaces that trust your namespace). Additionally, shared artifacts are included if you're using an artifact command. It is possible that the name you're using isn't unique. For example there might be two instances named \"mikal\" with different UUIDs. In that case, you will get an error indicating that there was more than one object which matched, and you'll need to use a UUID to refer to the object.","title":"Objects"},{"location":"user_guide/usage/","text":"Clients There is a command line client called \"sf-client\" deployed by ansible. It talks to Shaken Fist via a REST API. There is also a python API client library at shakenfist.clients.apiclient, which is what the command line client uses to call the API. The apiclient module also serves as useful example code for how to write your own client. The command line client can produce output in three formats: the standard \"pretty\" format, a mostly-csv format called \"simple\" (which is aimed at being easy to parse in shell scripts), and JSON. You select the output format with a flag like this: sf-client --simple instance list The default formatter is the \"pretty\" formatter, so you never need to specify that on the command line. You can explore what the command line client is capable of by asking it for help: sf-client --help Networking Virtual networks / micro segmentation is provided by VXLAN meshes between the instances. Hypervisors are joined to a given mesh when they start their first instance on that network. DHCP services are optionally offered from a \"network services\" node, which is just a hypervisor node with some extra dnsmasq process. NAT is also optionally available from the network services node. If your network provides NAT, it consumes an IP address from the floating IP pool to do so, and performs NAT in a network namespace on the network node. You create a network on the command line like this: sf-client network create mynet 192.168.1.0/24 Where \"192.168.1.0/24\" is the netblock to use, and \"mynet\" is the name of the network. You'll get back output describing the network, including the UUID of the network, which is used in later calls. Instances Every instance gets a config drive. Its always an ISO9660 drive. It's always the second virtual disk attached to the VM (vdb on Linux). There is no metadata server. Additionally, there is no image service -- you specify the image to use by providing a URL. That URL is cached, but can be to any HTTP server anywhere. Even better, there are no flavors. You specify what resources your instance should have at boot time and that's what you get. No more being forced into a t-shirt sized description of your needs. Instances are always cattle. Any feature that made instances feel like pets has not been implemented. That said, you can snapshot an instance. Snapshots aren't reliable backups, just like they're not really reliable backups on OpenStack. There is a small but real chance that a snapshot will contain an inconsistent state if you're snapshotting a busy database or something like that. One minor difference from OpenStack -- when you snapshot your instance you can snapshot all of the virtual disks (except the config drive) if you want to. Snapshots are delivered as files you can download via a mechanism external to Shaken Fist (for example an HTTP server pointed at the snapshot directory). You start an instance like this: sf-client instance create \"myinstance\" 1 2048 -d 8@cirros -n netuuid Where \"myinstance\" is the name of the instance, it has 1 vCPU, 2048MB of RAM, a single 8gb disk (more on this in a second) and a single network interface on the network with the UUID \"netuuid\". \"8 @cirros \" is a \"short disk specification\". These are in the form size@image, where the @image is optional. You can specify more than one disk, so this is valid: sf-client instance create \"myinstance\" 1 2048 -d 8@cirros -d 8 -d 8 -n netuuid In this case we have three disks, all of 8gb. The boot disk is imaged with cirros. The \"cirros\" here is shorthand. By default, you specify a URL for the image you want, so to boot a cirros instance you might use http://download.cirros-cloud.net/0.5.1/cirros-0.5.1-x86_64-disk.img -- that gets old though, so for common cloud images there is a shorthand format, where Shaken Fist knows how to generate the download URL from a short description. In this case \"cirros\" means \"the latest release of cirros\". You can also specify a version like this: sf-client instance create \"myinstance\" 1 2048 -d 8@cirros:0.5.1 -d 8 -d 8 -n netuuid \"Common cloud images\" is currently defined as cirros and Ubuntu. You can also use a \"detailed disk specification\", which is what fancy people use. Its syntax is similar: sf-client instance create \"myinstance\" 1 2048 -D size=8,base=cirros,bus=ide,type=cdrom -d 8 -d 8 -n netuuid The specification is composed of a series of key-value pairs. Valid keys are: size; base; bus; and type. If you don't specify a key, you'll get a reasonable default. Here's how the keys work: size as per the shorthand notation. base as per the shorthand notation, including version specification. bus is any valid disk bus for libvirt, which is virtio, ide, scsi, usb. Use virtio unless you have a really good reason otherwise -- the performance of the others are terrible. An example of a good reason is to install virtio drivers into legacy operating systems that lack them natively. type can be one of disk or cdrom. Note that cdroms are excluded from snapshots. Similarly, networks have a \"short network specification\", where you can specify the UUID of a network, but also optionally the IP address to use for the interface. You can also have more than one network interface, so this is valid: sf-client instance create \"myinstance\" 1 2048 -d 8@cirros -n netuuid1 \\ -n netuuid2@10.0.0.4 There is also a shorthand \"short network specification\" which implies immediately floating the interface. The details are the same as -n, except the flag is -f: sf-client instance create \"myinstance\" 1 2048 -d 8@cirros -f netuuid1 There is a \"detailed network specification\" as well, which is composed of the following keys: network_uuid is the UUID of the network to use. address is the IPv4 network address to use, if free. If its not free the instance will fail to start. If you don't want an address on this interface, use \"none\" as the value for address. If you do not specify any value for address, an address on the network will be assigned to you. macaddress the mac address to use for the interface. model is the model of the network device, with options being ne2k_isa, i82551, i82557b, i82559er, ne2k_pci, pcnet, rtl8139, e1000, and virtio. The default model is virtio. float if true indicates to immediately float the interface once the instance is created. Missing documentation I really should document these as well: nodes networks: delete, list instance: show, delete, list, ssh keys, user data, reboots (hard and soft), poweroff, poweron, pause, unpause, snapshot images: pre-caching metadata authentication Maybe one day I will.","title":"Usage"},{"location":"user_guide/usage/#clients","text":"There is a command line client called \"sf-client\" deployed by ansible. It talks to Shaken Fist via a REST API. There is also a python API client library at shakenfist.clients.apiclient, which is what the command line client uses to call the API. The apiclient module also serves as useful example code for how to write your own client. The command line client can produce output in three formats: the standard \"pretty\" format, a mostly-csv format called \"simple\" (which is aimed at being easy to parse in shell scripts), and JSON. You select the output format with a flag like this: sf-client --simple instance list The default formatter is the \"pretty\" formatter, so you never need to specify that on the command line. You can explore what the command line client is capable of by asking it for help: sf-client --help","title":"Clients"},{"location":"user_guide/usage/#networking","text":"Virtual networks / micro segmentation is provided by VXLAN meshes between the instances. Hypervisors are joined to a given mesh when they start their first instance on that network. DHCP services are optionally offered from a \"network services\" node, which is just a hypervisor node with some extra dnsmasq process. NAT is also optionally available from the network services node. If your network provides NAT, it consumes an IP address from the floating IP pool to do so, and performs NAT in a network namespace on the network node. You create a network on the command line like this: sf-client network create mynet 192.168.1.0/24 Where \"192.168.1.0/24\" is the netblock to use, and \"mynet\" is the name of the network. You'll get back output describing the network, including the UUID of the network, which is used in later calls.","title":"Networking"},{"location":"user_guide/usage/#instances","text":"Every instance gets a config drive. Its always an ISO9660 drive. It's always the second virtual disk attached to the VM (vdb on Linux). There is no metadata server. Additionally, there is no image service -- you specify the image to use by providing a URL. That URL is cached, but can be to any HTTP server anywhere. Even better, there are no flavors. You specify what resources your instance should have at boot time and that's what you get. No more being forced into a t-shirt sized description of your needs. Instances are always cattle. Any feature that made instances feel like pets has not been implemented. That said, you can snapshot an instance. Snapshots aren't reliable backups, just like they're not really reliable backups on OpenStack. There is a small but real chance that a snapshot will contain an inconsistent state if you're snapshotting a busy database or something like that. One minor difference from OpenStack -- when you snapshot your instance you can snapshot all of the virtual disks (except the config drive) if you want to. Snapshots are delivered as files you can download via a mechanism external to Shaken Fist (for example an HTTP server pointed at the snapshot directory). You start an instance like this: sf-client instance create \"myinstance\" 1 2048 -d 8@cirros -n netuuid Where \"myinstance\" is the name of the instance, it has 1 vCPU, 2048MB of RAM, a single 8gb disk (more on this in a second) and a single network interface on the network with the UUID \"netuuid\". \"8 @cirros \" is a \"short disk specification\". These are in the form size@image, where the @image is optional. You can specify more than one disk, so this is valid: sf-client instance create \"myinstance\" 1 2048 -d 8@cirros -d 8 -d 8 -n netuuid In this case we have three disks, all of 8gb. The boot disk is imaged with cirros. The \"cirros\" here is shorthand. By default, you specify a URL for the image you want, so to boot a cirros instance you might use http://download.cirros-cloud.net/0.5.1/cirros-0.5.1-x86_64-disk.img -- that gets old though, so for common cloud images there is a shorthand format, where Shaken Fist knows how to generate the download URL from a short description. In this case \"cirros\" means \"the latest release of cirros\". You can also specify a version like this: sf-client instance create \"myinstance\" 1 2048 -d 8@cirros:0.5.1 -d 8 -d 8 -n netuuid \"Common cloud images\" is currently defined as cirros and Ubuntu. You can also use a \"detailed disk specification\", which is what fancy people use. Its syntax is similar: sf-client instance create \"myinstance\" 1 2048 -D size=8,base=cirros,bus=ide,type=cdrom -d 8 -d 8 -n netuuid The specification is composed of a series of key-value pairs. Valid keys are: size; base; bus; and type. If you don't specify a key, you'll get a reasonable default. Here's how the keys work: size as per the shorthand notation. base as per the shorthand notation, including version specification. bus is any valid disk bus for libvirt, which is virtio, ide, scsi, usb. Use virtio unless you have a really good reason otherwise -- the performance of the others are terrible. An example of a good reason is to install virtio drivers into legacy operating systems that lack them natively. type can be one of disk or cdrom. Note that cdroms are excluded from snapshots. Similarly, networks have a \"short network specification\", where you can specify the UUID of a network, but also optionally the IP address to use for the interface. You can also have more than one network interface, so this is valid: sf-client instance create \"myinstance\" 1 2048 -d 8@cirros -n netuuid1 \\ -n netuuid2@10.0.0.4 There is also a shorthand \"short network specification\" which implies immediately floating the interface. The details are the same as -n, except the flag is -f: sf-client instance create \"myinstance\" 1 2048 -d 8@cirros -f netuuid1 There is a \"detailed network specification\" as well, which is composed of the following keys: network_uuid is the UUID of the network to use. address is the IPv4 network address to use, if free. If its not free the instance will fail to start. If you don't want an address on this interface, use \"none\" as the value for address. If you do not specify any value for address, an address on the network will be assigned to you. macaddress the mac address to use for the interface. model is the model of the network device, with options being ne2k_isa, i82551, i82557b, i82559er, ne2k_pci, pcnet, rtl8139, e1000, and virtio. The default model is virtio. float if true indicates to immediately float the interface once the instance is created.","title":"Instances"},{"location":"user_guide/usage/#missing-documentation","text":"I really should document these as well: nodes networks: delete, list instance: show, delete, list, ssh keys, user data, reboots (hard and soft), poweroff, poweron, pause, unpause, snapshot images: pre-caching metadata authentication Maybe one day I will.","title":"Missing documentation"}]}